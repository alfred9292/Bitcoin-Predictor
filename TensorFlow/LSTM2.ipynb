{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.contrib.learn as tflearn\n",
    "import tensorflow.contrib.layers as tflayers\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "import tensorflow.contrib.metrics as metrics\n",
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph=tf.get_default_graph()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 258.62  250.65  248.7 ]\n",
      " [ 255.41  253.43  249.99]\n",
      " [ 256.34  255.28  251.23]\n",
      " ...\n",
      " [3924.37 3881.57 3916.55]\n",
      " [3960.91 3885.59 3919.66]\n",
      " [4048.73 3902.41 3925.99]]\n"
     ]
    }
   ],
   "source": [
    "rnn_unit=32    \n",
    "lstm_layers=4     \n",
    "input_size=2\n",
    "output_size=1\n",
    "lr=0.0001\n",
    "df = read_csv('demo3.csv',header=0)       \n",
    "data=df.iloc[:,1:4].values  \n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(batch_size=32,time_step=2,train_begin=0,train_end=1200):\n",
    "    batch_index=[]\n",
    "    data_train=data[train_begin:train_end]\n",
    "    normalized_train_data=(data_train-np.mean(data_train,axis=0))/np.std(data_train,axis=0)  \n",
    "    train_x,train_y=[],[]   \n",
    "    for i in range(len(normalized_train_data)-time_step):\n",
    "       if i % batch_size==0:\n",
    "           batch_index.append(i)\n",
    "       x=normalized_train_data[i:i+time_step,:2]\n",
    "       y=normalized_train_data[i:i+time_step,2,np.newaxis]\n",
    "       train_x.append(x.tolist())\n",
    "       train_y.append(y.tolist())\n",
    "    batch_index.append((len(normalized_train_data)-time_step))\n",
    "    return batch_index,train_x,train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(time_step=2,test_begin=1200):\n",
    "    data_test=data[test_begin:]\n",
    "    mean=np.mean(data_test,axis=0)\n",
    "    std=np.std(data_test,axis=0)\n",
    "    normalized_test_data=(data_test-mean)/std  \n",
    "    size=(len(normalized_test_data)+time_step-1)//time_step  \n",
    "    test_x,test_y=[],[]\n",
    "    for i in range(size-1):\n",
    "       x=normalized_test_data[i*time_step:(i+1)*time_step,:2]\n",
    "       y=normalized_test_data[i*time_step:(i+1)*time_step,2]\n",
    "       test_x.append(x.tolist())\n",
    "       test_y.extend(y)\n",
    "    test_x.append((normalized_test_data[(i+1)*time_step:,:2]).tolist())\n",
    "    test_y.extend((normalized_test_data[(i+1)*time_step:,2]).tolist())\n",
    "    return mean,std,test_x,test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights={\n",
    "         'in':tf.Variable(tf.random_normal([input_size,rnn_unit])),\n",
    "         'out':tf.Variable(tf.random_normal([rnn_unit,1]))\n",
    "        }\n",
    "biases={\n",
    "        'in':tf.Variable(tf.constant(0.1,shape=[rnn_unit,])),\n",
    "        'out':tf.Variable(tf.constant(0.1,shape=[1,]))\n",
    "       }\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstmCell():\n",
    "    #basicLstm单元\n",
    "    basicLstm = tf.nn.rnn_cell.BasicLSTMCell(rnn_unit)\n",
    "    # dropout\n",
    "    drop = tf.nn.rnn_cell.DropoutWrapper(basicLstm, output_keep_prob=0.5)\n",
    "    return basicLstm\n",
    "\n",
    "def lstm(X):   \n",
    "    batch_size=tf.shape(X)[0]\n",
    "    time_step=tf.shape(X)[1]\n",
    "    w_in=weights['in']\n",
    "    b_in=biases['in']\n",
    "    input=tf.reshape(X,[-1,input_size])  \n",
    "    input_rnn=tf.matmul(input,w_in)+b_in\n",
    "    input_rnn=tf.reshape(input_rnn,[-1,time_step,rnn_unit])  \n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([lstmCell() for i in range(lstm_layers)])\n",
    "    init_state=cell.zero_state(batch_size,dtype=tf.float32)\n",
    "    output_rnn,final_states=tf.nn.dynamic_rnn(cell, input_rnn,initial_state=init_state, dtype=tf.float32)\n",
    "    output=tf.reshape(output_rnn,[-1,rnn_unit]) \n",
    "    w_out=weights['out']\n",
    "    b_out=biases['out']\n",
    "    pred=tf.matmul(output,w_out)+b_out\n",
    "    return pred,final_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 0  loss: 0.5060995\n",
      "Number of iterations: 1  loss: 0.47708806\n",
      "Number of iterations: 2  loss: 0.42931226\n",
      "Number of iterations: 3  loss: 0.3580785\n",
      "Number of iterations: 4  loss: 0.2691479\n",
      "Number of iterations: 5  loss: 0.18734305\n",
      "Number of iterations: 6  loss: 0.14635791\n",
      "Number of iterations: 7  loss: 0.15571325\n",
      "Number of iterations: 8  loss: 0.18948688\n",
      "Number of iterations: 9  loss: 0.21768638\n",
      "Number of iterations: 10  loss: 0.23082376\n",
      "Number of iterations: 11  loss: 0.23294744\n",
      "Number of iterations: 12  loss: 0.22924826\n",
      "Number of iterations: 13  loss: 0.22267333\n",
      "Number of iterations: 14  loss: 0.21459915\n",
      "Number of iterations: 15  loss: 0.20569004\n",
      "Number of iterations: 16  loss: 0.19633584\n",
      "Number of iterations: 17  loss: 0.18683062\n",
      "Number of iterations: 18  loss: 0.1774292\n",
      "Number of iterations: 19  loss: 0.16835035\n",
      "Number of iterations: 20  loss: 0.15975828\n",
      "Number of iterations: 21  loss: 0.15173857\n",
      "Number of iterations: 22  loss: 0.14428379\n",
      "Number of iterations: 23  loss: 0.1373013\n",
      "Number of iterations: 24  loss: 0.13064498\n",
      "Number of iterations: 25  loss: 0.12415398\n",
      "Number of iterations: 26  loss: 0.11767667\n",
      "Number of iterations: 27  loss: 0.11107284\n",
      "Number of iterations: 28  loss: 0.10420627\n",
      "Number of iterations: 29  loss: 0.096943386\n",
      "Number of iterations: 30  loss: 0.08916525\n",
      "Number of iterations: 31  loss: 0.08079792\n",
      "Number of iterations: 32  loss: 0.07187029\n",
      "Number of iterations: 33  loss: 0.06257258\n",
      "Number of iterations: 34  loss: 0.053198777\n",
      "Number of iterations: 35  loss: 0.044020366\n",
      "Number of iterations: 36  loss: 0.035339423\n",
      "Number of iterations: 37  loss: 0.027505608\n",
      "Number of iterations: 38  loss: 0.020827154\n",
      "Number of iterations: 39  loss: 0.015473137\n",
      "Number of iterations: 40  loss: 0.0114257615\n",
      "Number of iterations: 41  loss: 0.008495904\n",
      "Number of iterations: 42  loss: 0.006408203\n",
      "Number of iterations: 43  loss: 0.0048953043\n",
      "Number of iterations: 44  loss: 0.0037620473\n",
      "Number of iterations: 45  loss: 0.002890356\n",
      "Number of iterations: 46  loss: 0.0022171598\n",
      "Number of iterations: 47  loss: 0.0017034484\n",
      "Number of iterations: 48  loss: 0.0013196984\n",
      "Number of iterations: 49  loss: 0.0010379223\n",
      "Number of iterations: 50  loss: 0.0008332968\n",
      "Number of iterations: 51  loss: 0.0006836472\n",
      "Number of iterations: 52  loss: 0.0005719988\n",
      "Number of iterations: 53  loss: 0.00048529552\n",
      "Number of iterations: 54  loss: 0.00041533532\n",
      "Number of iterations: 55  loss: 0.00035653598\n",
      "Number of iterations: 56  loss: 0.0003063209\n",
      "Number of iterations: 57  loss: 0.00026288902\n",
      "Number of iterations: 58  loss: 0.00022596082\n",
      "Number of iterations: 59  loss: 0.00019480739\n",
      "Number of iterations: 60  loss: 0.00016965307\n",
      "Number of iterations: 61  loss: 0.00014966469\n",
      "Number of iterations: 62  loss: 0.00013510977\n",
      "Number of iterations: 63  loss: 0.000124783\n",
      "Number of iterations: 64  loss: 0.00011911506\n",
      "Number of iterations: 65  loss: 0.000116506475\n",
      "Number of iterations: 66  loss: 0.00011780137\n",
      "Number of iterations: 67  loss: 0.0001209267\n",
      "Number of iterations: 68  loss: 0.00012741228\n",
      "Number of iterations: 69  loss: 0.00013448788\n",
      "Number of iterations: 70  loss: 0.00014471124\n",
      "Number of iterations: 71  loss: 0.00015422342\n",
      "Number of iterations: 72  loss: 0.00016710926\n",
      "Number of iterations: 73  loss: 0.00017777251\n",
      "Number of iterations: 74  loss: 0.00019266029\n",
      "Number of iterations: 75  loss: 0.00020332719\n",
      "Number of iterations: 76  loss: 0.00022003053\n",
      "Number of iterations: 77  loss: 0.00022953577\n",
      "Number of iterations: 78  loss: 0.00024848158\n",
      "Number of iterations: 79  loss: 0.00025538736\n",
      "Number of iterations: 80  loss: 0.00027799356\n",
      "Number of iterations: 81  loss: 0.00028021462\n",
      "Number of iterations: 82  loss: 0.00030979217\n",
      "Number of iterations: 83  loss: 0.0003041635\n",
      "Number of iterations: 84  loss: 0.000348246\n",
      "Number of iterations: 85  loss: 0.00033063907\n",
      "Number of iterations: 86  loss: 0.0004074064\n",
      "Number of iterations: 87  loss: 0.00037654355\n",
      "Number of iterations: 88  loss: 0.000533552\n",
      "Number of iterations: 89  loss: 0.0005106262\n",
      "Number of iterations: 90  loss: 0.0008761889\n",
      "Number of iterations: 91  loss: 0.00096780097\n",
      "Number of iterations: 92  loss: 0.0018225882\n",
      "Number of iterations: 93  loss: 0.0022323995\n",
      "Number of iterations: 94  loss: 0.0036213712\n",
      "Number of iterations: 95  loss: 0.0038511793\n",
      "Number of iterations: 96  loss: 0.004272033\n",
      "Number of iterations: 97  loss: 0.0029607285\n",
      "Number of iterations: 98  loss: 0.0024544285\n",
      "Number of iterations: 99  loss: 0.0008542015\n",
      "Number of iterations: 100  loss: 0.0013542429\n",
      "Number of iterations: 101  loss: 0.000422864\n",
      "Number of iterations: 102  loss: 0.0010478749\n",
      "Number of iterations: 103  loss: 0.0004680628\n",
      "Number of iterations: 104  loss: 0.0008620532\n",
      "Number of iterations: 105  loss: 0.00049189245\n",
      "Number of iterations: 106  loss: 0.00072779466\n",
      "Number of iterations: 107  loss: 0.00050958415\n",
      "Number of iterations: 108  loss: 0.00065021095\n",
      "Number of iterations: 109  loss: 0.0005283252\n",
      "Number of iterations: 110  loss: 0.0006119477\n",
      "Number of iterations: 111  loss: 0.0005450901\n",
      "Number of iterations: 112  loss: 0.0005965705\n",
      "Number of iterations: 113  loss: 0.0005597728\n",
      "Number of iterations: 114  loss: 0.0005958749\n",
      "Number of iterations: 115  loss: 0.0005752322\n",
      "Number of iterations: 116  loss: 0.00060831395\n",
      "Number of iterations: 117  loss: 0.00059626705\n",
      "Number of iterations: 118  loss: 0.0006375042\n",
      "Number of iterations: 119  loss: 0.0006299958\n",
      "Number of iterations: 120  loss: 0.0006923234\n",
      "Number of iterations: 121  loss: 0.0006870847\n",
      "Number of iterations: 122  loss: 0.0007875808\n",
      "Number of iterations: 123  loss: 0.0007818887\n",
      "Number of iterations: 124  loss: 0.00094146497\n",
      "Number of iterations: 125  loss: 0.0009258942\n",
      "Number of iterations: 126  loss: 0.0011605026\n",
      "Number of iterations: 127  loss: 0.0011040291\n",
      "Number of iterations: 128  loss: 0.0014032981\n",
      "Number of iterations: 129  loss: 0.0012384086\n",
      "Number of iterations: 130  loss: 0.0015568042\n",
      "Number of iterations: 131  loss: 0.0012105458\n",
      "Number of iterations: 132  loss: 0.0015192349\n",
      "Number of iterations: 133  loss: 0.0010042217\n",
      "Number of iterations: 134  loss: 0.0013330384\n",
      "Number of iterations: 135  loss: 0.0007615892\n",
      "Number of iterations: 136  loss: 0.0011233946\n",
      "Number of iterations: 137  loss: 0.00060192693\n",
      "Number of iterations: 138  loss: 0.0009557658\n",
      "Number of iterations: 139  loss: 0.0005274937\n",
      "Number of iterations: 140  loss: 0.0008355322\n",
      "Number of iterations: 141  loss: 0.0005002874\n",
      "Number of iterations: 142  loss: 0.000754268\n",
      "Number of iterations: 143  loss: 0.0004945126\n",
      "Number of iterations: 144  loss: 0.0007043566\n",
      "Number of iterations: 145  loss: 0.00049851945\n",
      "Number of iterations: 146  loss: 0.0006797924\n",
      "Number of iterations: 147  loss: 0.0005080489\n",
      "Number of iterations: 148  loss: 0.0006764286\n",
      "Number of iterations: 149  loss: 0.0005223479\n",
      "Number of iterations: 150  loss: 0.0006922013\n",
      "Number of iterations: 151  loss: 0.0005423621\n",
      "Number of iterations: 152  loss: 0.00072680024\n",
      "Number of iterations: 153  loss: 0.00056953507\n",
      "Number of iterations: 154  loss: 0.0007805928\n",
      "Number of iterations: 155  loss: 0.00060459843\n",
      "Number of iterations: 156  loss: 0.000852623\n",
      "Number of iterations: 157  loss: 0.0006457494\n",
      "Number of iterations: 158  loss: 0.0009376376\n",
      "Number of iterations: 159  loss: 0.00068643177\n",
      "Number of iterations: 160  loss: 0.0010231909\n",
      "Number of iterations: 161  loss: 0.0007145808\n",
      "Number of iterations: 162  loss: 0.0010901226\n",
      "Number of iterations: 163  loss: 0.0007165122\n",
      "Number of iterations: 164  loss: 0.0011197273\n",
      "Number of iterations: 165  loss: 0.00068597525\n",
      "Number of iterations: 166  loss: 0.0011048159\n",
      "Number of iterations: 167  loss: 0.00063082034\n",
      "Number of iterations: 168  loss: 0.0010542923\n",
      "Number of iterations: 169  loss: 0.0005685091\n",
      "Number of iterations: 170  loss: 0.0009862494\n",
      "Number of iterations: 171  loss: 0.0005144183\n",
      "Number of iterations: 172  loss: 0.0009174556\n",
      "Number of iterations: 173  loss: 0.00047519643\n",
      "Number of iterations: 174  loss: 0.0008584047\n",
      "Number of iterations: 175  loss: 0.00045042348\n",
      "Number of iterations: 176  loss: 0.0008138736\n",
      "Number of iterations: 177  loss: 0.00043695467\n",
      "Number of iterations: 178  loss: 0.0007851891\n",
      "Number of iterations: 179  loss: 0.00043165826\n",
      "Number of iterations: 180  loss: 0.00077200524\n",
      "Number of iterations: 181  loss: 0.0004322936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 182  loss: 0.00077327096\n",
      "Number of iterations: 183  loss: 0.00043743406\n",
      "Number of iterations: 184  loss: 0.00078765413\n",
      "Number of iterations: 185  loss: 0.00044611047\n",
      "Number of iterations: 186  loss: 0.00081347674\n",
      "Number of iterations: 187  loss: 0.00045735986\n",
      "Number of iterations: 188  loss: 0.0008483931\n",
      "Number of iterations: 189  loss: 0.00046981446\n",
      "Number of iterations: 190  loss: 0.00088891096\n",
      "Number of iterations: 191  loss: 0.00048143504\n",
      "Number of iterations: 192  loss: 0.00093019876\n",
      "Number of iterations: 193  loss: 0.00048954325\n",
      "Number of iterations: 194  loss: 0.000966406\n",
      "Number of iterations: 195  loss: 0.0004914576\n",
      "Number of iterations: 196  loss: 0.0009918661\n",
      "Number of iterations: 197  loss: 0.00048546266\n",
      "Number of iterations: 198  loss: 0.001002718\n",
      "Number of iterations: 199  loss: 0.00047174844\n",
      "Number of iterations: 200  loss: 0.0009983066\n",
      "Number of iterations: 201  loss: 0.00045252353\n",
      "Number of iterations: 202  loss: 0.0009812893\n",
      "Number of iterations: 203  loss: 0.0004311331\n",
      "Number of iterations: 204  loss: 0.0009564323\n",
      "Number of iterations: 205  loss: 0.00041075193\n",
      "Number of iterations: 206  loss: 0.0009290361\n",
      "Number of iterations: 207  loss: 0.00039351825\n",
      "Number of iterations: 208  loss: 0.0009036136\n",
      "Number of iterations: 209  loss: 0.00038034856\n",
      "Number of iterations: 210  loss: 0.0008833588\n",
      "Number of iterations: 211  loss: 0.00037127273\n",
      "Number of iterations: 212  loss: 0.00087014964\n",
      "Number of iterations: 213  loss: 0.0003658508\n",
      "Number of iterations: 214  loss: 0.00086476427\n",
      "Number of iterations: 215  loss: 0.000363463\n",
      "Number of iterations: 216  loss: 0.0008671601\n",
      "Number of iterations: 217  loss: 0.0003634517\n",
      "Number of iterations: 218  loss: 0.00087665726\n",
      "Number of iterations: 219  loss: 0.00036514417\n",
      "Number of iterations: 220  loss: 0.000892026\n",
      "Number of iterations: 221  loss: 0.0003678125\n",
      "Number of iterations: 222  loss: 0.00091153674\n",
      "Number of iterations: 223  loss: 0.0003706547\n",
      "Number of iterations: 224  loss: 0.0009330322\n",
      "Number of iterations: 225  loss: 0.00037281387\n",
      "Number of iterations: 226  loss: 0.00095408637\n",
      "Number of iterations: 227  loss: 0.0003734529\n",
      "Number of iterations: 228  loss: 0.0009722971\n",
      "Number of iterations: 229  loss: 0.00037191532\n",
      "Number of iterations: 230  loss: 0.000985653\n",
      "Number of iterations: 231  loss: 0.00036791406\n",
      "Number of iterations: 232  loss: 0.0009929354\n",
      "Number of iterations: 233  loss: 0.00036159554\n",
      "Number of iterations: 234  loss: 0.0009939899\n",
      "Number of iterations: 235  loss: 0.00035354478\n",
      "Number of iterations: 236  loss: 0.0009896363\n",
      "Number of iterations: 237  loss: 0.0003445943\n",
      "Number of iterations: 238  loss: 0.0009814826\n",
      "Number of iterations: 239  loss: 0.00033562444\n",
      "Number of iterations: 240  loss: 0.0009714859\n",
      "Number of iterations: 241  loss: 0.0003273552\n",
      "Number of iterations: 242  loss: 0.00096158625\n",
      "Number of iterations: 243  loss: 0.0003202685\n",
      "Number of iterations: 244  loss: 0.00095341646\n",
      "Number of iterations: 245  loss: 0.0003145935\n",
      "Number of iterations: 246  loss: 0.0009481392\n",
      "Number of iterations: 247  loss: 0.00031036008\n",
      "Number of iterations: 248  loss: 0.0009464438\n",
      "Number of iterations: 249  loss: 0.00030742827\n",
      "Number of iterations: 250  loss: 0.0009485233\n",
      "Number of iterations: 251  loss: 0.00030558527\n",
      "Number of iterations: 252  loss: 0.0009541432\n",
      "Number of iterations: 253  loss: 0.0003045381\n",
      "Number of iterations: 254  loss: 0.00096273\n",
      "Number of iterations: 255  loss: 0.0003039642\n",
      "Number of iterations: 256  loss: 0.0009734086\n",
      "Number of iterations: 257  loss: 0.00030352236\n",
      "Number of iterations: 258  loss: 0.0009851324\n",
      "Number of iterations: 259  loss: 0.0003028708\n",
      "Number of iterations: 260  loss: 0.000996776\n",
      "Number of iterations: 261  loss: 0.00030171016\n",
      "Number of iterations: 262  loss: 0.001007256\n",
      "Number of iterations: 263  loss: 0.00029982676\n",
      "Number of iterations: 264  loss: 0.0010157118\n",
      "Number of iterations: 265  loss: 0.0002971183\n",
      "Number of iterations: 266  loss: 0.0010216046\n",
      "Number of iterations: 267  loss: 0.00029361047\n",
      "Number of iterations: 268  loss: 0.0010247569\n",
      "Number of iterations: 269  loss: 0.0002894546\n",
      "Number of iterations: 270  loss: 0.0010254129\n",
      "Number of iterations: 271  loss: 0.00028488282\n",
      "Number of iterations: 272  loss: 0.0010241221\n",
      "Number of iterations: 273  loss: 0.00028016506\n",
      "Number of iterations: 274  loss: 0.0010216417\n",
      "Number of iterations: 275  loss: 0.0002755629\n",
      "Number of iterations: 276  loss: 0.0010188062\n",
      "Number of iterations: 277  loss: 0.0002712798\n",
      "Number of iterations: 278  loss: 0.0010163896\n",
      "Number of iterations: 279  loss: 0.00026745524\n",
      "Number of iterations: 280  loss: 0.0010150344\n",
      "Number of iterations: 281  loss: 0.00026415352\n",
      "Number of iterations: 282  loss: 0.0010151932\n",
      "Number of iterations: 283  loss: 0.00026137626\n",
      "Number of iterations: 284  loss: 0.0010170579\n",
      "Number of iterations: 285  loss: 0.00025906105\n",
      "Number of iterations: 286  loss: 0.0010206442\n",
      "Number of iterations: 287  loss: 0.00025712056\n",
      "Number of iterations: 288  loss: 0.0010257392\n",
      "Number of iterations: 289  loss: 0.00025542307\n",
      "Number of iterations: 290  loss: 0.0010319973\n",
      "Number of iterations: 291  loss: 0.00025383921\n",
      "Number of iterations: 292  loss: 0.001038949\n",
      "Number of iterations: 293  loss: 0.00025223946\n",
      "Number of iterations: 294  loss: 0.0010460939\n",
      "Number of iterations: 295  loss: 0.000250506\n",
      "Number of iterations: 296  loss: 0.0010529376\n",
      "Number of iterations: 297  loss: 0.0002485534\n",
      "Number of iterations: 298  loss: 0.0010590683\n",
      "Number of iterations: 299  loss: 0.00024633275\n",
      "Number of iterations: 300  loss: 0.0010641947\n",
      "Number of iterations: 301  loss: 0.00024384134\n",
      "Number of iterations: 302  loss: 0.0010681772\n",
      "Number of iterations: 303  loss: 0.00024111704\n",
      "Number of iterations: 304  loss: 0.0010710632\n",
      "Number of iterations: 305  loss: 0.00023823135\n",
      "Number of iterations: 306  loss: 0.0010730197\n",
      "Number of iterations: 307  loss: 0.00023528244\n",
      "Number of iterations: 308  loss: 0.0010743457\n",
      "Number of iterations: 309  loss: 0.00023237147\n",
      "Number of iterations: 310  loss: 0.0010754076\n",
      "Number of iterations: 311  loss: 0.00022959209\n",
      "Number of iterations: 312  loss: 0.0010765657\n",
      "Number of iterations: 313  loss: 0.00022701992\n",
      "Number of iterations: 314  loss: 0.0010781537\n",
      "Number of iterations: 315  loss: 0.00022471075\n",
      "Number of iterations: 316  loss: 0.001080441\n",
      "Number of iterations: 317  loss: 0.00022268502\n",
      "Number of iterations: 318  loss: 0.0010835768\n",
      "Number of iterations: 319  loss: 0.00022094275\n",
      "Number of iterations: 320  loss: 0.0010876156\n",
      "Number of iterations: 321  loss: 0.00021945906\n",
      "Number of iterations: 322  loss: 0.0010924985\n",
      "Number of iterations: 323  loss: 0.0002181873\n",
      "Number of iterations: 324  loss: 0.0010980683\n",
      "Number of iterations: 325  loss: 0.0002170708\n",
      "Number of iterations: 326  loss: 0.0011040835\n",
      "Number of iterations: 327  loss: 0.00021604467\n",
      "Number of iterations: 328  loss: 0.0011102924\n",
      "Number of iterations: 329  loss: 0.00021504787\n",
      "Number of iterations: 330  loss: 0.0011164095\n",
      "Number of iterations: 331  loss: 0.00021403181\n",
      "Number of iterations: 332  loss: 0.0011221843\n",
      "Number of iterations: 333  loss: 0.00021295795\n",
      "Number of iterations: 334  loss: 0.0011274245\n",
      "Number of iterations: 335  loss: 0.00021180107\n",
      "Number of iterations: 336  loss: 0.0011319817\n",
      "Number of iterations: 337  loss: 0.00021056236\n",
      "Number of iterations: 338  loss: 0.0011358373\n",
      "Number of iterations: 339  loss: 0.00020925341\n",
      "Number of iterations: 340  loss: 0.0011390346\n",
      "Number of iterations: 341  loss: 0.00020790179\n",
      "Number of iterations: 342  loss: 0.0011416817\n",
      "Number of iterations: 343  loss: 0.00020653628\n",
      "Number of iterations: 344  loss: 0.0011439398\n",
      "Number of iterations: 345  loss: 0.00020519\n",
      "Number of iterations: 346  loss: 0.0011460042\n",
      "Number of iterations: 347  loss: 0.00020389433\n",
      "Number of iterations: 348  loss: 0.0011480529\n",
      "Number of iterations: 349  loss: 0.00020266834\n",
      "Number of iterations: 350  loss: 0.0011502531\n",
      "Number of iterations: 351  loss: 0.00020152288\n",
      "Number of iterations: 352  loss: 0.0011527382\n",
      "Number of iterations: 353  loss: 0.00020046221\n",
      "Number of iterations: 354  loss: 0.0011555608\n",
      "Number of iterations: 355  loss: 0.0001994817\n",
      "Number of iterations: 356  loss: 0.0011587561\n",
      "Number of iterations: 357  loss: 0.0001985665\n",
      "Number of iterations: 358  loss: 0.0011622951\n",
      "Number of iterations: 359  loss: 0.00019770255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 360  loss: 0.0011661041\n",
      "Number of iterations: 361  loss: 0.00019686551\n",
      "Number of iterations: 362  loss: 0.001170084\n",
      "Number of iterations: 363  loss: 0.00019603691\n",
      "Number of iterations: 364  loss: 0.001174106\n",
      "Number of iterations: 365  loss: 0.00019519657\n",
      "Number of iterations: 366  loss: 0.0011780659\n",
      "Number of iterations: 367  loss: 0.00019433118\n",
      "Number of iterations: 368  loss: 0.0011818538\n",
      "Number of iterations: 369  loss: 0.00019343129\n",
      "Number of iterations: 370  loss: 0.0011853812\n",
      "Number of iterations: 371  loss: 0.00019249499\n",
      "Number of iterations: 372  loss: 0.0011886071\n",
      "Number of iterations: 373  loss: 0.0001915254\n",
      "Number of iterations: 374  loss: 0.0011915246\n",
      "Number of iterations: 375  loss: 0.00019052876\n",
      "Number of iterations: 376  loss: 0.0011941673\n",
      "Number of iterations: 377  loss: 0.00018951822\n",
      "Number of iterations: 378  loss: 0.0011965762\n",
      "Number of iterations: 379  loss: 0.00018850576\n",
      "Number of iterations: 380  loss: 0.0011988397\n",
      "Number of iterations: 381  loss: 0.00018750395\n",
      "Number of iterations: 382  loss: 0.0012010232\n",
      "Number of iterations: 383  loss: 0.00018652144\n",
      "Number of iterations: 384  loss: 0.0012031989\n",
      "Number of iterations: 385  loss: 0.0001855695\n",
      "Number of iterations: 386  loss: 0.0012054536\n",
      "Number of iterations: 387  loss: 0.00018464967\n",
      "Number of iterations: 388  loss: 0.001207832\n",
      "Number of iterations: 389  loss: 0.00018376451\n",
      "Number of iterations: 390  loss: 0.0012103573\n",
      "Number of iterations: 391  loss: 0.00018291119\n",
      "Number of iterations: 392  loss: 0.0012130343\n",
      "Number of iterations: 393  loss: 0.00018208593\n",
      "Number of iterations: 394  loss: 0.0012158485\n",
      "Number of iterations: 395  loss: 0.0001812829\n",
      "Number of iterations: 396  loss: 0.0012187759\n",
      "Number of iterations: 397  loss: 0.00018049512\n",
      "Number of iterations: 398  loss: 0.0012217611\n",
      "Number of iterations: 399  loss: 0.00017971339\n",
      "Number of iterations: 400  loss: 0.0012247356\n",
      "Number of iterations: 401  loss: 0.00017893431\n",
      "Number of iterations: 402  loss: 0.0012276743\n",
      "Number of iterations: 403  loss: 0.00017815191\n",
      "Number of iterations: 404  loss: 0.0012305214\n",
      "Number of iterations: 405  loss: 0.0001773647\n",
      "Number of iterations: 406  loss: 0.0012332515\n",
      "Number of iterations: 407  loss: 0.00017656964\n",
      "Number of iterations: 408  loss: 0.0012358278\n",
      "Number of iterations: 409  loss: 0.0001757725\n",
      "Number of iterations: 410  loss: 0.0012382761\n",
      "Number of iterations: 411  loss: 0.00017497163\n",
      "Number of iterations: 412  loss: 0.0012405888\n",
      "Number of iterations: 413  loss: 0.00017417381\n",
      "Number of iterations: 414  loss: 0.0012428059\n",
      "Number of iterations: 415  loss: 0.00017338067\n",
      "Number of iterations: 416  loss: 0.001244941\n",
      "Number of iterations: 417  loss: 0.0001726002\n",
      "Number of iterations: 418  loss: 0.0012470427\n",
      "Number of iterations: 419  loss: 0.00017183104\n",
      "Number of iterations: 420  loss: 0.001249137\n",
      "Number of iterations: 421  loss: 0.0001710791\n",
      "Number of iterations: 422  loss: 0.001251254\n",
      "Number of iterations: 423  loss: 0.00017034494\n",
      "Number of iterations: 424  loss: 0.0012534173\n",
      "Number of iterations: 425  loss: 0.00016962946\n",
      "Number of iterations: 426  loss: 0.0012556439\n",
      "Number of iterations: 427  loss: 0.0001689305\n",
      "Number of iterations: 428  loss: 0.0012579358\n",
      "Number of iterations: 429  loss: 0.00016824638\n",
      "Number of iterations: 430  loss: 0.0012602836\n",
      "Number of iterations: 431  loss: 0.00016757683\n",
      "Number of iterations: 432  loss: 0.0012626688\n",
      "Number of iterations: 433  loss: 0.00016691694\n",
      "Number of iterations: 434  loss: 0.001265066\n",
      "Number of iterations: 435  loss: 0.00016626634\n",
      "Number of iterations: 436  loss: 0.0012674613\n",
      "Number of iterations: 437  loss: 0.0001656214\n",
      "Number of iterations: 438  loss: 0.0012698362\n",
      "Number of iterations: 439  loss: 0.0001649816\n",
      "Number of iterations: 440  loss: 0.001272151\n",
      "Number of iterations: 441  loss: 0.00016434355\n",
      "Number of iterations: 442  loss: 0.0012744081\n",
      "Number of iterations: 443  loss: 0.00016370988\n",
      "Number of iterations: 444  loss: 0.0012765953\n",
      "Number of iterations: 445  loss: 0.00016307987\n",
      "Number of iterations: 446  loss: 0.001278708\n",
      "Number of iterations: 447  loss: 0.00016245585\n",
      "Number of iterations: 448  loss: 0.0012807695\n",
      "Number of iterations: 449  loss: 0.00016183688\n",
      "Number of iterations: 450  loss: 0.001282776\n",
      "Number of iterations: 451  loss: 0.00016122658\n",
      "Number of iterations: 452  loss: 0.0012847454\n",
      "Number of iterations: 453  loss: 0.0001606249\n",
      "Number of iterations: 454  loss: 0.0012866941\n",
      "Number of iterations: 455  loss: 0.00016003275\n",
      "Number of iterations: 456  loss: 0.0012886383\n",
      "Number of iterations: 457  loss: 0.00015945337\n",
      "Number of iterations: 458  loss: 0.0012905927\n",
      "Number of iterations: 459  loss: 0.00015888573\n",
      "Number of iterations: 460  loss: 0.0012925632\n",
      "Number of iterations: 461  loss: 0.0001583275\n",
      "Number of iterations: 462  loss: 0.0012945583\n",
      "Number of iterations: 463  loss: 0.00015778045\n",
      "Number of iterations: 464  loss: 0.0012965788\n",
      "Number of iterations: 465  loss: 0.00015724463\n",
      "Number of iterations: 466  loss: 0.0012986215\n",
      "Number of iterations: 467  loss: 0.00015671775\n",
      "Number of iterations: 468  loss: 0.0013006678\n",
      "Number of iterations: 469  loss: 0.00015619898\n",
      "Number of iterations: 470  loss: 0.0013027211\n",
      "Number of iterations: 471  loss: 0.00015568803\n",
      "Number of iterations: 472  loss: 0.0013047738\n",
      "Number of iterations: 473  loss: 0.00015518465\n",
      "Number of iterations: 474  loss: 0.001306804\n",
      "Number of iterations: 475  loss: 0.00015468722\n",
      "Number of iterations: 476  loss: 0.0013087998\n",
      "Number of iterations: 477  loss: 0.000154195\n",
      "Number of iterations: 478  loss: 0.0013107678\n",
      "Number of iterations: 479  loss: 0.00015370933\n",
      "Number of iterations: 480  loss: 0.0013127018\n",
      "Number of iterations: 481  loss: 0.00015322996\n",
      "Number of iterations: 482  loss: 0.0013145931\n",
      "Number of iterations: 483  loss: 0.00015275671\n",
      "Number of iterations: 484  loss: 0.0013164582\n",
      "Number of iterations: 485  loss: 0.0001522906\n",
      "Number of iterations: 486  loss: 0.0013182918\n",
      "Number of iterations: 487  loss: 0.00015183198\n",
      "Number of iterations: 488  loss: 0.0013201082\n",
      "Number of iterations: 489  loss: 0.0001513818\n",
      "Number of iterations: 490  loss: 0.0013219051\n",
      "Number of iterations: 491  loss: 0.00015093948\n",
      "Number of iterations: 492  loss: 0.0013236934\n",
      "Number of iterations: 493  loss: 0.00015050682\n",
      "Number of iterations: 494  loss: 0.0013254834\n",
      "Number of iterations: 495  loss: 0.00015008093\n",
      "Number of iterations: 496  loss: 0.0013272752\n",
      "Number of iterations: 497  loss: 0.00014966316\n",
      "Number of iterations: 498  loss: 0.0013290651\n",
      "Number of iterations: 499  loss: 0.00014925389\n",
      "Number of iterations: 500  loss: 0.0013308463\n",
      "Number of iterations: 501  loss: 0.00014885118\n",
      "Number of iterations: 502  loss: 0.001332627\n",
      "Number of iterations: 503  loss: 0.00014845733\n",
      "Number of iterations: 504  loss: 0.0013344126\n",
      "Number of iterations: 505  loss: 0.00014806968\n",
      "Number of iterations: 506  loss: 0.0013361828\n",
      "Number of iterations: 507  loss: 0.00014768905\n",
      "Number of iterations: 508  loss: 0.0013379337\n",
      "Number of iterations: 509  loss: 0.00014731528\n",
      "Number of iterations: 510  loss: 0.0013396571\n",
      "Number of iterations: 511  loss: 0.00014694748\n",
      "Number of iterations: 512  loss: 0.0013413578\n",
      "Number of iterations: 513  loss: 0.00014658572\n",
      "Number of iterations: 514  loss: 0.0013430306\n",
      "Number of iterations: 515  loss: 0.00014623035\n",
      "Number of iterations: 516  loss: 0.0013446737\n",
      "Number of iterations: 517  loss: 0.00014588224\n",
      "Number of iterations: 518  loss: 0.0013462955\n",
      "Number of iterations: 519  loss: 0.00014554073\n",
      "Number of iterations: 520  loss: 0.0013478848\n",
      "Number of iterations: 521  loss: 0.000145206\n",
      "Number of iterations: 522  loss: 0.0013494635\n",
      "Number of iterations: 523  loss: 0.00014487829\n",
      "Number of iterations: 524  loss: 0.001351013\n",
      "Number of iterations: 525  loss: 0.00014455627\n",
      "Number of iterations: 526  loss: 0.0013525382\n",
      "Number of iterations: 527  loss: 0.00014424337\n",
      "Number of iterations: 528  loss: 0.001354061\n",
      "Number of iterations: 529  loss: 0.00014393576\n",
      "Number of iterations: 530  loss: 0.0013555652\n",
      "Number of iterations: 531  loss: 0.00014363563\n",
      "Number of iterations: 532  loss: 0.0013570586\n",
      "Number of iterations: 533  loss: 0.00014334322\n",
      "Number of iterations: 534  loss: 0.001358548\n",
      "Number of iterations: 535  loss: 0.00014305719\n",
      "Number of iterations: 536  loss: 0.0013600345\n",
      "Number of iterations: 537  loss: 0.00014277811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 538  loss: 0.0013615115\n",
      "Number of iterations: 539  loss: 0.00014250666\n",
      "Number of iterations: 540  loss: 0.0013629786\n",
      "Number of iterations: 541  loss: 0.0001422418\n",
      "Number of iterations: 542  loss: 0.001364434\n",
      "Number of iterations: 543  loss: 0.00014198334\n",
      "Number of iterations: 544  loss: 0.0013658829\n",
      "Number of iterations: 545  loss: 0.00014173044\n",
      "Number of iterations: 546  loss: 0.001367322\n",
      "Number of iterations: 547  loss: 0.00014148476\n",
      "Number of iterations: 548  loss: 0.001368748\n",
      "Number of iterations: 549  loss: 0.00014124635\n",
      "Number of iterations: 550  loss: 0.0013701689\n",
      "Number of iterations: 551  loss: 0.00014101458\n",
      "Number of iterations: 552  loss: 0.0013715718\n",
      "Number of iterations: 553  loss: 0.00014078927\n",
      "Number of iterations: 554  loss: 0.0013729695\n",
      "Number of iterations: 555  loss: 0.00014057037\n",
      "Number of iterations: 556  loss: 0.0013743589\n",
      "Number of iterations: 557  loss: 0.0001403583\n",
      "Number of iterations: 558  loss: 0.0013757292\n",
      "Number of iterations: 559  loss: 0.0001401532\n",
      "Number of iterations: 560  loss: 0.0013771007\n",
      "Number of iterations: 561  loss: 0.00013995457\n",
      "Number of iterations: 562  loss: 0.0013784663\n",
      "Number of iterations: 563  loss: 0.00013976249\n",
      "Number of iterations: 564  loss: 0.0013798177\n",
      "Number of iterations: 565  loss: 0.00013957794\n",
      "Number of iterations: 566  loss: 0.0013811764\n",
      "Number of iterations: 567  loss: 0.00013940001\n",
      "Number of iterations: 568  loss: 0.0013825281\n",
      "Number of iterations: 569  loss: 0.0001392283\n",
      "Number of iterations: 570  loss: 0.0013838767\n",
      "Number of iterations: 571  loss: 0.00013906378\n",
      "Number of iterations: 572  loss: 0.0013852279\n",
      "Number of iterations: 573  loss: 0.00013890592\n",
      "Number of iterations: 574  loss: 0.0013865713\n",
      "Number of iterations: 575  loss: 0.00013875458\n",
      "Number of iterations: 576  loss: 0.001387921\n",
      "Number of iterations: 577  loss: 0.00013860977\n",
      "Number of iterations: 578  loss: 0.0013892622\n",
      "Number of iterations: 579  loss: 0.00013847239\n",
      "Number of iterations: 580  loss: 0.0013905979\n",
      "Number of iterations: 581  loss: 0.00013834167\n",
      "Number of iterations: 582  loss: 0.0013919367\n",
      "Number of iterations: 583  loss: 0.00013821757\n",
      "Number of iterations: 584  loss: 0.0013932682\n",
      "Number of iterations: 585  loss: 0.00013809987\n",
      "Number of iterations: 586  loss: 0.001394604\n",
      "Number of iterations: 587  loss: 0.00013798894\n",
      "Number of iterations: 588  loss: 0.0013959324\n",
      "Number of iterations: 589  loss: 0.00013788354\n",
      "Number of iterations: 590  loss: 0.0013972629\n",
      "Number of iterations: 591  loss: 0.00013778676\n",
      "Number of iterations: 592  loss: 0.0013985914\n",
      "Number of iterations: 593  loss: 0.00013769532\n",
      "Number of iterations: 594  loss: 0.001399917\n",
      "Number of iterations: 595  loss: 0.00013761148\n",
      "Number of iterations: 596  loss: 0.0014012364\n",
      "Number of iterations: 597  loss: 0.00013753363\n",
      "Number of iterations: 598  loss: 0.0014025426\n",
      "Number of iterations: 599  loss: 0.00013746294\n",
      "Number of iterations: 600  loss: 0.0014038481\n",
      "Number of iterations: 601  loss: 0.00013739828\n",
      "Number of iterations: 602  loss: 0.0014051524\n",
      "Number of iterations: 603  loss: 0.00013734108\n",
      "Number of iterations: 604  loss: 0.0014064506\n",
      "Number of iterations: 605  loss: 0.00013728988\n",
      "Number of iterations: 606  loss: 0.0014077475\n",
      "Number of iterations: 607  loss: 0.00013724552\n",
      "Number of iterations: 608  loss: 0.0014090476\n",
      "Number of iterations: 609  loss: 0.00013720807\n",
      "Number of iterations: 610  loss: 0.001410345\n",
      "Number of iterations: 611  loss: 0.00013717721\n",
      "Number of iterations: 612  loss: 0.0014116379\n",
      "Number of iterations: 613  loss: 0.0001371524\n",
      "Number of iterations: 614  loss: 0.0014129273\n",
      "Number of iterations: 615  loss: 0.00013713437\n",
      "Number of iterations: 616  loss: 0.0014142186\n",
      "Number of iterations: 617  loss: 0.0001371229\n",
      "Number of iterations: 618  loss: 0.0014155015\n",
      "Number of iterations: 619  loss: 0.00013711782\n",
      "Number of iterations: 620  loss: 0.0014167879\n",
      "Number of iterations: 621  loss: 0.00013711888\n",
      "Number of iterations: 622  loss: 0.0014180725\n",
      "Number of iterations: 623  loss: 0.0001371278\n",
      "Number of iterations: 624  loss: 0.0014193588\n",
      "Number of iterations: 625  loss: 0.00013714118\n",
      "Number of iterations: 626  loss: 0.0014206389\n",
      "Number of iterations: 627  loss: 0.00013716154\n",
      "Number of iterations: 628  loss: 0.0014219193\n",
      "Number of iterations: 629  loss: 0.00013718873\n",
      "Number of iterations: 630  loss: 0.0014231907\n",
      "Number of iterations: 631  loss: 0.0001372223\n",
      "Number of iterations: 632  loss: 0.001424468\n",
      "Number of iterations: 633  loss: 0.00013726235\n",
      "Number of iterations: 634  loss: 0.0014257289\n",
      "Number of iterations: 635  loss: 0.00013730825\n",
      "Number of iterations: 636  loss: 0.0014269948\n",
      "Number of iterations: 637  loss: 0.00013736116\n",
      "Number of iterations: 638  loss: 0.0014282509\n",
      "Number of iterations: 639  loss: 0.00013741877\n",
      "Number of iterations: 640  loss: 0.0014295082\n",
      "Number of iterations: 641  loss: 0.0001374851\n",
      "Number of iterations: 642  loss: 0.0014307661\n",
      "Number of iterations: 643  loss: 0.0001375567\n",
      "Number of iterations: 644  loss: 0.0014320121\n",
      "Number of iterations: 645  loss: 0.00013763379\n",
      "Number of iterations: 646  loss: 0.0014332586\n",
      "Number of iterations: 647  loss: 0.00013771806\n",
      "Number of iterations: 648  loss: 0.0014345068\n",
      "Number of iterations: 649  loss: 0.00013780734\n",
      "Number of iterations: 650  loss: 0.001435747\n",
      "Number of iterations: 651  loss: 0.00013790322\n",
      "Number of iterations: 652  loss: 0.0014369839\n",
      "Number of iterations: 653  loss: 0.0001380055\n",
      "Number of iterations: 654  loss: 0.0014382247\n",
      "Number of iterations: 655  loss: 0.00013811296\n",
      "Number of iterations: 656  loss: 0.0014394606\n",
      "Number of iterations: 657  loss: 0.00013822722\n",
      "Number of iterations: 658  loss: 0.0014406877\n",
      "Number of iterations: 659  loss: 0.00013834686\n",
      "Number of iterations: 660  loss: 0.0014419127\n",
      "Number of iterations: 661  loss: 0.00013847275\n",
      "Number of iterations: 662  loss: 0.0014431378\n",
      "Number of iterations: 663  loss: 0.00013860455\n",
      "Number of iterations: 664  loss: 0.0014443516\n",
      "Number of iterations: 665  loss: 0.00013874235\n",
      "Number of iterations: 666  loss: 0.0014455594\n",
      "Number of iterations: 667  loss: 0.0001388866\n",
      "Number of iterations: 668  loss: 0.0014467619\n",
      "Number of iterations: 669  loss: 0.00013903642\n",
      "Number of iterations: 670  loss: 0.001447962\n",
      "Number of iterations: 671  loss: 0.00013919079\n",
      "Number of iterations: 672  loss: 0.0014491677\n",
      "Number of iterations: 673  loss: 0.00013935265\n",
      "Number of iterations: 674  loss: 0.0014503617\n",
      "Number of iterations: 675  loss: 0.00013951895\n",
      "Number of iterations: 676  loss: 0.0014515605\n",
      "Number of iterations: 677  loss: 0.00013969172\n",
      "Number of iterations: 678  loss: 0.0014527644\n",
      "Number of iterations: 679  loss: 0.00013986915\n",
      "Number of iterations: 680  loss: 0.0014539514\n",
      "Number of iterations: 681  loss: 0.0001400529\n",
      "Number of iterations: 682  loss: 0.0014551458\n",
      "Number of iterations: 683  loss: 0.00014024127\n",
      "Number of iterations: 684  loss: 0.0014563336\n",
      "Number of iterations: 685  loss: 0.00014043614\n",
      "Number of iterations: 686  loss: 0.0014575318\n",
      "Number of iterations: 687  loss: 0.00014063485\n",
      "Number of iterations: 688  loss: 0.0014587156\n",
      "Number of iterations: 689  loss: 0.00014084054\n",
      "Number of iterations: 690  loss: 0.001459904\n",
      "Number of iterations: 691  loss: 0.00014105048\n",
      "Number of iterations: 692  loss: 0.0014610869\n",
      "Number of iterations: 693  loss: 0.00014126669\n",
      "Number of iterations: 694  loss: 0.001462266\n",
      "Number of iterations: 695  loss: 0.00014148765\n",
      "Number of iterations: 696  loss: 0.0014634341\n",
      "Number of iterations: 697  loss: 0.0001417147\n",
      "Number of iterations: 698  loss: 0.001464597\n",
      "Number of iterations: 699  loss: 0.00014194653\n",
      "Number of iterations: 700  loss: 0.001465757\n",
      "Number of iterations: 701  loss: 0.00014218387\n",
      "Number of iterations: 702  loss: 0.0014669026\n",
      "Number of iterations: 703  loss: 0.00014242555\n",
      "Number of iterations: 704  loss: 0.0014680568\n",
      "Number of iterations: 705  loss: 0.0001426735\n",
      "Number of iterations: 706  loss: 0.0014692013\n",
      "Number of iterations: 707  loss: 0.00014292498\n",
      "Number of iterations: 708  loss: 0.0014703531\n",
      "Number of iterations: 709  loss: 0.00014318197\n",
      "Number of iterations: 710  loss: 0.0014714991\n",
      "Number of iterations: 711  loss: 0.00014344405\n",
      "Number of iterations: 712  loss: 0.0014726376\n",
      "Number of iterations: 713  loss: 0.0001437119\n",
      "Number of iterations: 714  loss: 0.0014737761\n",
      "Number of iterations: 715  loss: 0.0001439831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 716  loss: 0.0014749172\n",
      "Number of iterations: 717  loss: 0.00014425987\n",
      "Number of iterations: 718  loss: 0.0014760478\n",
      "Number of iterations: 719  loss: 0.000144541\n",
      "Number of iterations: 720  loss: 0.0014771803\n",
      "Number of iterations: 721  loss: 0.0001448264\n",
      "Number of iterations: 722  loss: 0.0014783145\n",
      "Number of iterations: 723  loss: 0.00014511692\n",
      "Number of iterations: 724  loss: 0.0014794394\n",
      "Number of iterations: 725  loss: 0.00014541017\n",
      "Number of iterations: 726  loss: 0.0014805661\n",
      "Number of iterations: 727  loss: 0.00014570999\n",
      "Number of iterations: 728  loss: 0.0014816852\n",
      "Number of iterations: 729  loss: 0.00014601389\n",
      "Number of iterations: 730  loss: 0.001482807\n",
      "Number of iterations: 731  loss: 0.00014632157\n",
      "Number of iterations: 732  loss: 0.0014839184\n",
      "Number of iterations: 733  loss: 0.00014663425\n",
      "Number of iterations: 734  loss: 0.001485018\n",
      "Number of iterations: 735  loss: 0.00014695211\n",
      "Number of iterations: 736  loss: 0.0014861189\n",
      "Number of iterations: 737  loss: 0.00014727438\n",
      "Number of iterations: 738  loss: 0.0014871994\n",
      "Number of iterations: 739  loss: 0.00014760178\n",
      "Number of iterations: 740  loss: 0.0014882719\n",
      "Number of iterations: 741  loss: 0.00014793208\n",
      "Number of iterations: 742  loss: 0.0014893526\n",
      "Number of iterations: 743  loss: 0.00014826751\n",
      "Number of iterations: 744  loss: 0.00149043\n",
      "Number of iterations: 745  loss: 0.00014860739\n",
      "Number of iterations: 746  loss: 0.0014914953\n",
      "Number of iterations: 747  loss: 0.00014894965\n",
      "Number of iterations: 748  loss: 0.0014925655\n",
      "Number of iterations: 749  loss: 0.00014929613\n",
      "Number of iterations: 750  loss: 0.0014936352\n",
      "Number of iterations: 751  loss: 0.0001496469\n",
      "Number of iterations: 752  loss: 0.0014946919\n",
      "Number of iterations: 753  loss: 0.00015000276\n",
      "Number of iterations: 754  loss: 0.0014957468\n",
      "Number of iterations: 755  loss: 0.00015036113\n",
      "Number of iterations: 756  loss: 0.0014967897\n",
      "Number of iterations: 757  loss: 0.00015072396\n",
      "Number of iterations: 758  loss: 0.0014978378\n",
      "Number of iterations: 759  loss: 0.00015109028\n",
      "Number of iterations: 760  loss: 0.0014988737\n",
      "Number of iterations: 761  loss: 0.00015145978\n",
      "Number of iterations: 762  loss: 0.0014999126\n",
      "Number of iterations: 763  loss: 0.00015183198\n",
      "Number of iterations: 764  loss: 0.0015009412\n",
      "Number of iterations: 765  loss: 0.00015220973\n",
      "Number of iterations: 766  loss: 0.0015019665\n",
      "Number of iterations: 767  loss: 0.00015258994\n",
      "Number of iterations: 768  loss: 0.0015029886\n",
      "Number of iterations: 769  loss: 0.00015297465\n",
      "Number of iterations: 770  loss: 0.0015039968\n",
      "Number of iterations: 771  loss: 0.00015336435\n",
      "Number of iterations: 772  loss: 0.0015049971\n",
      "Number of iterations: 773  loss: 0.0001537557\n",
      "Number of iterations: 774  loss: 0.0015059939\n",
      "Number of iterations: 775  loss: 0.00015415039\n",
      "Number of iterations: 776  loss: 0.0015069834\n",
      "Number of iterations: 777  loss: 0.00015455035\n",
      "Number of iterations: 778  loss: 0.0015079702\n",
      "Number of iterations: 779  loss: 0.00015495361\n",
      "Number of iterations: 780  loss: 0.0015089522\n",
      "Number of iterations: 781  loss: 0.00015535834\n",
      "Number of iterations: 782  loss: 0.0015099256\n",
      "Number of iterations: 783  loss: 0.00015576693\n",
      "Number of iterations: 784  loss: 0.0015108999\n",
      "Number of iterations: 785  loss: 0.00015617805\n",
      "Number of iterations: 786  loss: 0.0015118745\n",
      "Number of iterations: 787  loss: 0.0001565925\n",
      "Number of iterations: 788  loss: 0.0015128528\n",
      "Number of iterations: 789  loss: 0.00015701004\n",
      "Number of iterations: 790  loss: 0.0015138276\n",
      "Number of iterations: 791  loss: 0.00015742898\n",
      "Number of iterations: 792  loss: 0.0015147969\n",
      "Number of iterations: 793  loss: 0.00015785253\n",
      "Number of iterations: 794  loss: 0.0015157653\n",
      "Number of iterations: 795  loss: 0.00015827669\n",
      "Number of iterations: 796  loss: 0.0015167298\n",
      "Number of iterations: 797  loss: 0.00015870741\n",
      "Number of iterations: 798  loss: 0.0015176918\n",
      "Number of iterations: 799  loss: 0.00015913906\n",
      "Number of iterations: 800  loss: 0.0015186494\n",
      "Number of iterations: 801  loss: 0.00015957405\n",
      "Number of iterations: 802  loss: 0.0015196024\n",
      "Number of iterations: 803  loss: 0.00016001133\n",
      "Number of iterations: 804  loss: 0.0015205427\n",
      "Number of iterations: 805  loss: 0.00016045284\n",
      "Number of iterations: 806  loss: 0.0015214855\n",
      "Number of iterations: 807  loss: 0.00016089615\n",
      "Number of iterations: 808  loss: 0.0015224194\n",
      "Number of iterations: 809  loss: 0.00016134362\n",
      "Number of iterations: 810  loss: 0.0015233489\n",
      "Number of iterations: 811  loss: 0.00016179391\n",
      "Number of iterations: 812  loss: 0.0015242734\n",
      "Number of iterations: 813  loss: 0.00016224828\n",
      "Number of iterations: 814  loss: 0.0015251794\n",
      "Number of iterations: 815  loss: 0.00016270558\n",
      "Number of iterations: 816  loss: 0.0015260878\n",
      "Number of iterations: 817  loss: 0.00016316553\n",
      "Number of iterations: 818  loss: 0.0015269888\n",
      "Number of iterations: 819  loss: 0.0001636293\n",
      "Number of iterations: 820  loss: 0.001527889\n",
      "Number of iterations: 821  loss: 0.00016409515\n",
      "Number of iterations: 822  loss: 0.0015287821\n",
      "Number of iterations: 823  loss: 0.00016456425\n",
      "Number of iterations: 824  loss: 0.0015296655\n",
      "Number of iterations: 825  loss: 0.00016503502\n",
      "Number of iterations: 826  loss: 0.0015305498\n",
      "Number of iterations: 827  loss: 0.00016551022\n",
      "Number of iterations: 828  loss: 0.0015314317\n",
      "Number of iterations: 829  loss: 0.00016598616\n",
      "Number of iterations: 830  loss: 0.001532316\n",
      "Number of iterations: 831  loss: 0.00016646393\n",
      "Number of iterations: 832  loss: 0.0015331941\n",
      "Number of iterations: 833  loss: 0.00016694392\n",
      "Number of iterations: 834  loss: 0.001534067\n",
      "Number of iterations: 835  loss: 0.00016742754\n",
      "Number of iterations: 836  loss: 0.0015349401\n",
      "Number of iterations: 837  loss: 0.00016791195\n",
      "Number of iterations: 838  loss: 0.0015358133\n",
      "Number of iterations: 839  loss: 0.00016840025\n",
      "Number of iterations: 840  loss: 0.0015366754\n",
      "Number of iterations: 841  loss: 0.00016888972\n",
      "Number of iterations: 842  loss: 0.0015375371\n",
      "Number of iterations: 843  loss: 0.00016938311\n",
      "Number of iterations: 844  loss: 0.0015383943\n",
      "Number of iterations: 845  loss: 0.00016987683\n",
      "Number of iterations: 846  loss: 0.0015392482\n",
      "Number of iterations: 847  loss: 0.00017037125\n",
      "Number of iterations: 848  loss: 0.0015400929\n",
      "Number of iterations: 849  loss: 0.0001708718\n",
      "Number of iterations: 850  loss: 0.0015409287\n",
      "Number of iterations: 851  loss: 0.00017137395\n",
      "Number of iterations: 852  loss: 0.001541758\n",
      "Number of iterations: 853  loss: 0.00017187871\n",
      "Number of iterations: 854  loss: 0.0015425811\n",
      "Number of iterations: 855  loss: 0.00017238471\n",
      "Number of iterations: 856  loss: 0.0015433911\n",
      "Number of iterations: 857  loss: 0.00017289465\n",
      "Number of iterations: 858  loss: 0.0015442057\n",
      "Number of iterations: 859  loss: 0.00017340512\n",
      "Number of iterations: 860  loss: 0.0015450092\n",
      "Number of iterations: 861  loss: 0.00017391783\n",
      "Number of iterations: 862  loss: 0.0015458068\n",
      "Number of iterations: 863  loss: 0.00017443296\n",
      "Number of iterations: 864  loss: 0.0015466072\n",
      "Number of iterations: 865  loss: 0.00017494881\n",
      "Number of iterations: 866  loss: 0.0015474049\n",
      "Number of iterations: 867  loss: 0.00017546739\n",
      "Number of iterations: 868  loss: 0.0015481866\n",
      "Number of iterations: 869  loss: 0.0001759873\n",
      "Number of iterations: 870  loss: 0.0015489733\n",
      "Number of iterations: 871  loss: 0.00017650968\n",
      "Number of iterations: 872  loss: 0.001549764\n",
      "Number of iterations: 873  loss: 0.00017703233\n",
      "Number of iterations: 874  loss: 0.0015505423\n",
      "Number of iterations: 875  loss: 0.00017755762\n",
      "Number of iterations: 876  loss: 0.0015513207\n",
      "Number of iterations: 877  loss: 0.00017808421\n",
      "Number of iterations: 878  loss: 0.0015520938\n",
      "Number of iterations: 879  loss: 0.00017861338\n",
      "Number of iterations: 880  loss: 0.0015528548\n",
      "Number of iterations: 881  loss: 0.00017914602\n",
      "Number of iterations: 882  loss: 0.001553599\n",
      "Number of iterations: 883  loss: 0.00017968063\n",
      "Number of iterations: 884  loss: 0.001554345\n",
      "Number of iterations: 885  loss: 0.00018021489\n",
      "Number of iterations: 886  loss: 0.0015550876\n",
      "Number of iterations: 887  loss: 0.00018075206\n",
      "Number of iterations: 888  loss: 0.001555815\n",
      "Number of iterations: 889  loss: 0.00018129092\n",
      "Number of iterations: 890  loss: 0.0015565463\n",
      "Number of iterations: 891  loss: 0.00018183129\n",
      "Number of iterations: 892  loss: 0.0015572754\n",
      "Number of iterations: 893  loss: 0.0001823745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 894  loss: 0.0015579896\n",
      "Number of iterations: 895  loss: 0.00018291992\n",
      "Number of iterations: 896  loss: 0.0015587019\n",
      "Number of iterations: 897  loss: 0.00018346419\n",
      "Number of iterations: 898  loss: 0.0015594218\n",
      "Number of iterations: 899  loss: 0.00018401362\n",
      "Number of iterations: 900  loss: 0.0015601234\n",
      "Number of iterations: 901  loss: 0.00018456124\n",
      "Number of iterations: 902  loss: 0.0015608276\n",
      "Number of iterations: 903  loss: 0.00018511254\n",
      "Number of iterations: 904  loss: 0.001561515\n",
      "Number of iterations: 905  loss: 0.00018566506\n",
      "Number of iterations: 906  loss: 0.0015622104\n",
      "Number of iterations: 907  loss: 0.00018621758\n",
      "Number of iterations: 908  loss: 0.0015628962\n",
      "Number of iterations: 909  loss: 0.00018677172\n",
      "Number of iterations: 910  loss: 0.0015635754\n",
      "Number of iterations: 911  loss: 0.00018732858\n",
      "Number of iterations: 912  loss: 0.0015642537\n",
      "Number of iterations: 913  loss: 0.00018788622\n",
      "Number of iterations: 914  loss: 0.0015649286\n",
      "Number of iterations: 915  loss: 0.00018844414\n",
      "Number of iterations: 916  loss: 0.0015655934\n",
      "Number of iterations: 917  loss: 0.00018900656\n",
      "Number of iterations: 918  loss: 0.0015662514\n",
      "Number of iterations: 919  loss: 0.00018956746\n",
      "Number of iterations: 920  loss: 0.0015669159\n",
      "Number of iterations: 921  loss: 0.00019012857\n",
      "Number of iterations: 922  loss: 0.0015675739\n",
      "Number of iterations: 923  loss: 0.00019069425\n",
      "Number of iterations: 924  loss: 0.0015682217\n",
      "Number of iterations: 925  loss: 0.00019125977\n",
      "Number of iterations: 926  loss: 0.00156887\n",
      "Number of iterations: 927  loss: 0.00019182668\n",
      "Number of iterations: 928  loss: 0.0015695086\n",
      "Number of iterations: 929  loss: 0.00019239681\n",
      "Number of iterations: 930  loss: 0.0015701345\n",
      "Number of iterations: 931  loss: 0.00019296708\n",
      "Number of iterations: 932  loss: 0.0015707602\n",
      "Number of iterations: 933  loss: 0.00019353876\n",
      "Number of iterations: 934  loss: 0.0015713783\n",
      "Number of iterations: 935  loss: 0.0001941124\n",
      "Number of iterations: 936  loss: 0.001571994\n",
      "Number of iterations: 937  loss: 0.00019468718\n",
      "Number of iterations: 938  loss: 0.0015726046\n",
      "Number of iterations: 939  loss: 0.00019525996\n",
      "Number of iterations: 940  loss: 0.0015732144\n",
      "Number of iterations: 941  loss: 0.00019583489\n",
      "Number of iterations: 942  loss: 0.0015738228\n",
      "Number of iterations: 943  loss: 0.0001964129\n",
      "Number of iterations: 944  loss: 0.0015744213\n",
      "Number of iterations: 945  loss: 0.00019698971\n",
      "Number of iterations: 946  loss: 0.0015750218\n",
      "Number of iterations: 947  loss: 0.00019756786\n",
      "Number of iterations: 948  loss: 0.0015756318\n",
      "Number of iterations: 949  loss: 0.00019814714\n",
      "Number of iterations: 950  loss: 0.0015762196\n",
      "Number of iterations: 951  loss: 0.00019872797\n",
      "Number of iterations: 952  loss: 0.0015768089\n",
      "Number of iterations: 953  loss: 0.0001993121\n",
      "Number of iterations: 954  loss: 0.0015773872\n",
      "Number of iterations: 955  loss: 0.00019989515\n",
      "Number of iterations: 956  loss: 0.0015779622\n",
      "Number of iterations: 957  loss: 0.00020047906\n",
      "Number of iterations: 958  loss: 0.0015785365\n",
      "Number of iterations: 959  loss: 0.0002010636\n",
      "Number of iterations: 960  loss: 0.0015791005\n",
      "Number of iterations: 961  loss: 0.00020164835\n",
      "Number of iterations: 962  loss: 0.0015796747\n",
      "Number of iterations: 963  loss: 0.00020223639\n",
      "Number of iterations: 964  loss: 0.0015802396\n",
      "Number of iterations: 965  loss: 0.00020282653\n",
      "Number of iterations: 966  loss: 0.0015807942\n",
      "Number of iterations: 967  loss: 0.00020341444\n",
      "Number of iterations: 968  loss: 0.0015813448\n",
      "Number of iterations: 969  loss: 0.00020400465\n",
      "Number of iterations: 970  loss: 0.0015818932\n",
      "Number of iterations: 971  loss: 0.00020459585\n",
      "Number of iterations: 972  loss: 0.0015824427\n",
      "Number of iterations: 973  loss: 0.00020518724\n",
      "Number of iterations: 974  loss: 0.001582988\n",
      "Number of iterations: 975  loss: 0.00020577876\n",
      "Number of iterations: 976  loss: 0.0015835279\n",
      "Number of iterations: 977  loss: 0.0002063731\n",
      "Number of iterations: 978  loss: 0.0015840627\n",
      "Number of iterations: 979  loss: 0.00020696704\n",
      "Number of iterations: 980  loss: 0.0015845959\n",
      "Number of iterations: 981  loss: 0.0002075618\n",
      "Number of iterations: 982  loss: 0.0015851354\n",
      "Number of iterations: 983  loss: 0.00020815812\n",
      "Number of iterations: 984  loss: 0.0015856618\n",
      "Number of iterations: 985  loss: 0.00020875352\n",
      "Number of iterations: 986  loss: 0.0015861818\n",
      "Number of iterations: 987  loss: 0.00020935028\n",
      "Number of iterations: 988  loss: 0.001586706\n",
      "Number of iterations: 989  loss: 0.00020994751\n",
      "Number of iterations: 990  loss: 0.0015872248\n",
      "Number of iterations: 991  loss: 0.00021054612\n",
      "Number of iterations: 992  loss: 0.001587739\n",
      "Number of iterations: 993  loss: 0.00021114477\n",
      "Number of iterations: 994  loss: 0.0015882469\n",
      "Number of iterations: 995  loss: 0.00021174503\n",
      "Number of iterations: 996  loss: 0.0015887531\n",
      "Number of iterations: 997  loss: 0.00021234564\n",
      "Number of iterations: 998  loss: 0.0015892561\n",
      "Number of iterations: 999  loss: 0.00021294654\n",
      "Number of iterations: 1000  loss: 0.0015897611\n",
      "Number of iterations: 1001  loss: 0.00021354792\n",
      "Number of iterations: 1002  loss: 0.0015902611\n",
      "Number of iterations: 1003  loss: 0.00021415002\n",
      "Number of iterations: 1004  loss: 0.0015907623\n",
      "Number of iterations: 1005  loss: 0.00021475334\n",
      "Number of iterations: 1006  loss: 0.0015912565\n",
      "Number of iterations: 1007  loss: 0.00021535624\n",
      "Number of iterations: 1008  loss: 0.0015917552\n",
      "Number of iterations: 1009  loss: 0.00021595947\n",
      "Number of iterations: 1010  loss: 0.0015922382\n",
      "Number of iterations: 1011  loss: 0.00021656569\n",
      "Number of iterations: 1012  loss: 0.0015927207\n",
      "Number of iterations: 1013  loss: 0.00021716858\n",
      "Number of iterations: 1014  loss: 0.0015932061\n",
      "Number of iterations: 1015  loss: 0.00021777501\n",
      "Number of iterations: 1016  loss: 0.0015936864\n",
      "Number of iterations: 1017  loss: 0.00021838277\n",
      "Number of iterations: 1018  loss: 0.0015941657\n",
      "Number of iterations: 1019  loss: 0.00021898898\n",
      "Number of iterations: 1020  loss: 0.0015946423\n",
      "Number of iterations: 1021  loss: 0.00021959736\n",
      "Number of iterations: 1022  loss: 0.0015951179\n",
      "Number of iterations: 1023  loss: 0.00022020626\n",
      "Number of iterations: 1024  loss: 0.001595589\n",
      "Number of iterations: 1025  loss: 0.00022081433\n",
      "Number of iterations: 1026  loss: 0.0015960584\n",
      "Number of iterations: 1027  loss: 0.00022142522\n",
      "Number of iterations: 1028  loss: 0.0015965232\n",
      "Number of iterations: 1029  loss: 0.00022203512\n",
      "Number of iterations: 1030  loss: 0.0015969909\n",
      "Number of iterations: 1031  loss: 0.00022264557\n",
      "Number of iterations: 1032  loss: 0.001597454\n",
      "Number of iterations: 1033  loss: 0.00022325639\n",
      "Number of iterations: 1034  loss: 0.0015979193\n",
      "Number of iterations: 1035  loss: 0.00022386549\n",
      "Number of iterations: 1036  loss: 0.0015983898\n",
      "Number of iterations: 1037  loss: 0.000224478\n",
      "Number of iterations: 1038  loss: 0.001598853\n",
      "Number of iterations: 1039  loss: 0.00022509132\n",
      "Number of iterations: 1040  loss: 0.0015993161\n",
      "Number of iterations: 1041  loss: 0.0002257028\n",
      "Number of iterations: 1042  loss: 0.0015997804\n",
      "Number of iterations: 1043  loss: 0.00022631661\n",
      "Number of iterations: 1044  loss: 0.0016002349\n",
      "Number of iterations: 1045  loss: 0.00022693102\n",
      "Number of iterations: 1046  loss: 0.0016006954\n",
      "Number of iterations: 1047  loss: 0.00022754453\n",
      "Number of iterations: 1048  loss: 0.0016011484\n",
      "Number of iterations: 1049  loss: 0.0002281596\n",
      "Number of iterations: 1050  loss: 0.0016016007\n",
      "Number of iterations: 1051  loss: 0.00022877648\n",
      "Number of iterations: 1052  loss: 0.0016020463\n",
      "Number of iterations: 1053  loss: 0.00022939526\n",
      "Number of iterations: 1054  loss: 0.0016024829\n",
      "Number of iterations: 1055  loss: 0.00023001325\n",
      "Number of iterations: 1056  loss: 0.0016029178\n",
      "Number of iterations: 1057  loss: 0.00023063358\n",
      "Number of iterations: 1058  loss: 0.0016033475\n",
      "Number of iterations: 1059  loss: 0.00023125317\n",
      "Number of iterations: 1060  loss: 0.0016037769\n",
      "Number of iterations: 1061  loss: 0.00023187159\n",
      "Number of iterations: 1062  loss: 0.0016042104\n",
      "Number of iterations: 1063  loss: 0.00023249314\n",
      "Number of iterations: 1064  loss: 0.0016046392\n",
      "Number of iterations: 1065  loss: 0.00023311253\n",
      "Number of iterations: 1066  loss: 0.0016050678\n",
      "Number of iterations: 1067  loss: 0.00023373294\n",
      "Number of iterations: 1068  loss: 0.0016054979\n",
      "Number of iterations: 1069  loss: 0.00023435503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 1070  loss: 0.0016059267\n",
      "Number of iterations: 1071  loss: 0.00023497526\n",
      "Number of iterations: 1072  loss: 0.001606358\n",
      "Number of iterations: 1073  loss: 0.00023559589\n",
      "Number of iterations: 1074  loss: 0.0016067906\n",
      "Number of iterations: 1075  loss: 0.00023621709\n",
      "Number of iterations: 1076  loss: 0.0016072188\n",
      "Number of iterations: 1077  loss: 0.00023683836\n",
      "Number of iterations: 1078  loss: 0.0016076477\n",
      "Number of iterations: 1079  loss: 0.00023745978\n",
      "Number of iterations: 1080  loss: 0.0016080684\n",
      "Number of iterations: 1081  loss: 0.00023808434\n",
      "Number of iterations: 1082  loss: 0.001608479\n",
      "Number of iterations: 1083  loss: 0.00023871085\n",
      "Number of iterations: 1084  loss: 0.0016088941\n",
      "Number of iterations: 1085  loss: 0.00023933484\n",
      "Number of iterations: 1086  loss: 0.0016092984\n",
      "Number of iterations: 1087  loss: 0.0002399588\n",
      "Number of iterations: 1088  loss: 0.0016097008\n",
      "Number of iterations: 1089  loss: 0.00024058581\n",
      "Number of iterations: 1090  loss: 0.0016101004\n",
      "Number of iterations: 1091  loss: 0.00024121189\n",
      "Number of iterations: 1092  loss: 0.0016104998\n",
      "Number of iterations: 1093  loss: 0.00024183987\n",
      "Number of iterations: 1094  loss: 0.0016108983\n",
      "Number of iterations: 1095  loss: 0.00024246755\n",
      "Number of iterations: 1096  loss: 0.001611289\n",
      "Number of iterations: 1097  loss: 0.00024309437\n",
      "Number of iterations: 1098  loss: 0.0016116807\n",
      "Number of iterations: 1099  loss: 0.00024372402\n",
      "Number of iterations: 1100  loss: 0.001612075\n",
      "Number of iterations: 1101  loss: 0.00024435337\n",
      "Number of iterations: 1102  loss: 0.0016124557\n",
      "Number of iterations: 1103  loss: 0.0002449818\n",
      "Number of iterations: 1104  loss: 0.001612843\n",
      "Number of iterations: 1105  loss: 0.00024560973\n",
      "Number of iterations: 1106  loss: 0.0016132364\n",
      "Number of iterations: 1107  loss: 0.00024623898\n",
      "Number of iterations: 1108  loss: 0.0016136179\n",
      "Number of iterations: 1109  loss: 0.00024686893\n",
      "Number of iterations: 1110  loss: 0.0016140107\n",
      "Number of iterations: 1111  loss: 0.00024749714\n",
      "Number of iterations: 1112  loss: 0.0016143968\n",
      "Number of iterations: 1113  loss: 0.00024812648\n",
      "Number of iterations: 1114  loss: 0.0016147731\n",
      "Number of iterations: 1115  loss: 0.0002487558\n",
      "Number of iterations: 1116  loss: 0.0016151565\n",
      "Number of iterations: 1117  loss: 0.0002493853\n",
      "Number of iterations: 1118  loss: 0.0016155337\n",
      "Number of iterations: 1119  loss: 0.0002500172\n",
      "Number of iterations: 1120  loss: 0.0016159092\n",
      "Number of iterations: 1121  loss: 0.00025064722\n",
      "Number of iterations: 1122  loss: 0.0016162831\n",
      "Number of iterations: 1123  loss: 0.00025127962\n",
      "Number of iterations: 1124  loss: 0.0016166504\n",
      "Number of iterations: 1125  loss: 0.00025191152\n",
      "Number of iterations: 1126  loss: 0.0016170117\n",
      "Number of iterations: 1127  loss: 0.00025254444\n",
      "Number of iterations: 1128  loss: 0.0016173702\n",
      "Number of iterations: 1129  loss: 0.00025317897\n",
      "Number of iterations: 1130  loss: 0.001617734\n",
      "Number of iterations: 1131  loss: 0.0002538122\n",
      "Number of iterations: 1132  loss: 0.0016180923\n",
      "Number of iterations: 1133  loss: 0.00025444425\n",
      "Number of iterations: 1134  loss: 0.001618455\n",
      "Number of iterations: 1135  loss: 0.0002550767\n",
      "Number of iterations: 1136  loss: 0.0016188101\n",
      "Number of iterations: 1137  loss: 0.00025570943\n",
      "Number of iterations: 1138  loss: 0.0016191633\n",
      "Number of iterations: 1139  loss: 0.00025634555\n",
      "Number of iterations: 1140  loss: 0.0016195226\n",
      "Number of iterations: 1141  loss: 0.00025697716\n",
      "Number of iterations: 1142  loss: 0.0016198786\n",
      "Number of iterations: 1143  loss: 0.0002576112\n",
      "Number of iterations: 1144  loss: 0.0016202306\n",
      "Number of iterations: 1145  loss: 0.00025824434\n",
      "Number of iterations: 1146  loss: 0.0016205797\n",
      "Number of iterations: 1147  loss: 0.00025887875\n",
      "Number of iterations: 1148  loss: 0.0016209338\n",
      "Number of iterations: 1149  loss: 0.00025951123\n",
      "Number of iterations: 1150  loss: 0.0016212814\n",
      "Number of iterations: 1151  loss: 0.00026014508\n",
      "Number of iterations: 1152  loss: 0.0016216317\n",
      "Number of iterations: 1153  loss: 0.00026078167\n",
      "Number of iterations: 1154  loss: 0.0016219763\n",
      "Number of iterations: 1155  loss: 0.00026141503\n",
      "Number of iterations: 1156  loss: 0.0016223213\n",
      "Number of iterations: 1157  loss: 0.00026205162\n",
      "Number of iterations: 1158  loss: 0.0016226614\n",
      "Number of iterations: 1159  loss: 0.00026268885\n",
      "Number of iterations: 1160  loss: 0.0016229945\n",
      "Number of iterations: 1161  loss: 0.00026332622\n",
      "Number of iterations: 1162  loss: 0.0016233315\n",
      "Number of iterations: 1163  loss: 0.00026396036\n",
      "Number of iterations: 1164  loss: 0.0016236686\n",
      "Number of iterations: 1165  loss: 0.0002645979\n",
      "Number of iterations: 1166  loss: 0.0016240074\n",
      "Number of iterations: 1167  loss: 0.00026523438\n",
      "Number of iterations: 1168  loss: 0.001624337\n",
      "Number of iterations: 1169  loss: 0.0002658745\n",
      "Number of iterations: 1170  loss: 0.0016246673\n",
      "Number of iterations: 1171  loss: 0.00026651015\n",
      "Number of iterations: 1172  loss: 0.0016250081\n",
      "Number of iterations: 1173  loss: 0.00026714566\n",
      "Number of iterations: 1174  loss: 0.0016253361\n",
      "Number of iterations: 1175  loss: 0.00026778455\n",
      "Number of iterations: 1176  loss: 0.0016256653\n",
      "Number of iterations: 1177  loss: 0.0002684226\n",
      "Number of iterations: 1178  loss: 0.0016259894\n",
      "Number of iterations: 1179  loss: 0.00026906203\n",
      "Number of iterations: 1180  loss: 0.0016263135\n",
      "Number of iterations: 1181  loss: 0.00026970168\n",
      "Number of iterations: 1182  loss: 0.0016266403\n",
      "Number of iterations: 1183  loss: 0.00027033794\n",
      "Number of iterations: 1184  loss: 0.0016269645\n",
      "Number of iterations: 1185  loss: 0.0002709756\n",
      "Number of iterations: 1186  loss: 0.0016272829\n",
      "Number of iterations: 1187  loss: 0.0002716146\n",
      "Number of iterations: 1188  loss: 0.0016275999\n",
      "Number of iterations: 1189  loss: 0.00027225306\n",
      "Number of iterations: 1190  loss: 0.0016279098\n",
      "Number of iterations: 1191  loss: 0.0002728932\n",
      "Number of iterations: 1192  loss: 0.0016282283\n",
      "Number of iterations: 1193  loss: 0.0002735296\n",
      "Number of iterations: 1194  loss: 0.0016285402\n",
      "Number of iterations: 1195  loss: 0.00027416818\n",
      "Number of iterations: 1196  loss: 0.0016288592\n",
      "Number of iterations: 1197  loss: 0.00027480716\n",
      "Number of iterations: 1198  loss: 0.0016291839\n",
      "Number of iterations: 1199  loss: 0.00027544628\n",
      "Number of iterations: 1200  loss: 0.0016294951\n",
      "Number of iterations: 1201  loss: 0.00027608467\n",
      "Number of iterations: 1202  loss: 0.0016298136\n",
      "Number of iterations: 1203  loss: 0.00027672487\n",
      "Number of iterations: 1204  loss: 0.0016301266\n",
      "Number of iterations: 1205  loss: 0.00027736538\n",
      "Number of iterations: 1206  loss: 0.0016304337\n",
      "Number of iterations: 1207  loss: 0.0002780034\n",
      "Number of iterations: 1208  loss: 0.0016307445\n",
      "Number of iterations: 1209  loss: 0.0002786461\n",
      "Number of iterations: 1210  loss: 0.001631046\n",
      "Number of iterations: 1211  loss: 0.00027928423\n",
      "Number of iterations: 1212  loss: 0.0016313458\n",
      "Number of iterations: 1213  loss: 0.00027992483\n",
      "Number of iterations: 1214  loss: 0.001631643\n",
      "Number of iterations: 1215  loss: 0.00028056532\n",
      "Number of iterations: 1216  loss: 0.0016319475\n",
      "Number of iterations: 1217  loss: 0.0002812067\n",
      "Number of iterations: 1218  loss: 0.0016322413\n",
      "Number of iterations: 1219  loss: 0.00028184778\n",
      "Number of iterations: 1220  loss: 0.0016325328\n",
      "Number of iterations: 1221  loss: 0.00028248722\n",
      "Number of iterations: 1222  loss: 0.0016328295\n",
      "Number of iterations: 1223  loss: 0.00028312855\n",
      "Number of iterations: 1224  loss: 0.0016331265\n",
      "Number of iterations: 1225  loss: 0.00028376823\n",
      "Number of iterations: 1226  loss: 0.0016334187\n",
      "Number of iterations: 1227  loss: 0.0002844114\n",
      "Number of iterations: 1228  loss: 0.0016337007\n",
      "Number of iterations: 1229  loss: 0.0002850505\n",
      "Number of iterations: 1230  loss: 0.0016339951\n",
      "Number of iterations: 1231  loss: 0.00028569292\n",
      "Number of iterations: 1232  loss: 0.0016342752\n",
      "Number of iterations: 1233  loss: 0.00028633312\n",
      "Number of iterations: 1234  loss: 0.0016345611\n",
      "Number of iterations: 1235  loss: 0.00028697352\n",
      "Number of iterations: 1236  loss: 0.0016348326\n",
      "Number of iterations: 1237  loss: 0.00028761657\n",
      "Number of iterations: 1238  loss: 0.0016351135\n",
      "Number of iterations: 1239  loss: 0.00028825836\n",
      "Number of iterations: 1240  loss: 0.0016353866\n",
      "Number of iterations: 1241  loss: 0.00028889973\n",
      "Number of iterations: 1242  loss: 0.0016356651\n",
      "Number of iterations: 1243  loss: 0.00028954088\n",
      "Number of iterations: 1244  loss: 0.0016359442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 1245  loss: 0.0002901792\n",
      "Number of iterations: 1246  loss: 0.0016362065\n",
      "Number of iterations: 1247  loss: 0.0002908183\n",
      "Number of iterations: 1248  loss: 0.001636478\n",
      "Number of iterations: 1249  loss: 0.00029146014\n",
      "Number of iterations: 1250  loss: 0.0016367447\n",
      "Number of iterations: 1251  loss: 0.00029209928\n",
      "Number of iterations: 1252  loss: 0.0016370107\n",
      "Number of iterations: 1253  loss: 0.0002927403\n",
      "Number of iterations: 1254  loss: 0.001637272\n",
      "Number of iterations: 1255  loss: 0.0002933823\n",
      "Number of iterations: 1256  loss: 0.0016375333\n",
      "Number of iterations: 1257  loss: 0.0002940191\n",
      "Number of iterations: 1258  loss: 0.0016377912\n",
      "Number of iterations: 1259  loss: 0.00029465937\n",
      "Number of iterations: 1260  loss: 0.0016380473\n",
      "Number of iterations: 1261  loss: 0.0002952987\n",
      "Number of iterations: 1262  loss: 0.0016382962\n",
      "Number of iterations: 1263  loss: 0.00029593616\n",
      "Number of iterations: 1264  loss: 0.0016385533\n",
      "Number of iterations: 1265  loss: 0.00029657592\n",
      "Number of iterations: 1266  loss: 0.0016387972\n",
      "Number of iterations: 1267  loss: 0.00029721568\n",
      "Number of iterations: 1268  loss: 0.0016390383\n",
      "Number of iterations: 1269  loss: 0.00029785422\n",
      "Number of iterations: 1270  loss: 0.0016392773\n",
      "Number of iterations: 1271  loss: 0.00029849348\n",
      "Number of iterations: 1272  loss: 0.0016395104\n",
      "Number of iterations: 1273  loss: 0.00029913237\n",
      "Number of iterations: 1274  loss: 0.0016397467\n",
      "Number of iterations: 1275  loss: 0.00029976675\n",
      "Number of iterations: 1276  loss: 0.0016399713\n",
      "Number of iterations: 1277  loss: 0.0003004058\n",
      "Number of iterations: 1278  loss: 0.0016402052\n",
      "Number of iterations: 1279  loss: 0.0003010402\n",
      "Number of iterations: 1280  loss: 0.0016404303\n",
      "Number of iterations: 1281  loss: 0.00030167686\n",
      "Number of iterations: 1282  loss: 0.0016406522\n",
      "Number of iterations: 1283  loss: 0.00030231095\n",
      "Number of iterations: 1284  loss: 0.0016408765\n",
      "Number of iterations: 1285  loss: 0.00030294652\n",
      "Number of iterations: 1286  loss: 0.0016410897\n",
      "Number of iterations: 1287  loss: 0.00030358197\n",
      "Number of iterations: 1288  loss: 0.0016413083\n",
      "Number of iterations: 1289  loss: 0.00030421373\n",
      "Number of iterations: 1290  loss: 0.0016415147\n",
      "Number of iterations: 1291  loss: 0.0003048449\n",
      "Number of iterations: 1292  loss: 0.0016417204\n",
      "Number of iterations: 1293  loss: 0.0003054776\n",
      "Number of iterations: 1294  loss: 0.0016419216\n",
      "Number of iterations: 1295  loss: 0.0003061067\n",
      "Number of iterations: 1296  loss: 0.001642113\n",
      "Number of iterations: 1297  loss: 0.0003067385\n",
      "Number of iterations: 1298  loss: 0.0016423067\n",
      "Number of iterations: 1299  loss: 0.0003073683\n",
      "Number of iterations: 1300  loss: 0.0016425034\n",
      "Number of iterations: 1301  loss: 0.0003079962\n",
      "Number of iterations: 1302  loss: 0.001642686\n",
      "Number of iterations: 1303  loss: 0.0003086243\n",
      "Number of iterations: 1304  loss: 0.0016428746\n",
      "Number of iterations: 1305  loss: 0.00030924915\n",
      "Number of iterations: 1306  loss: 0.0016430423\n",
      "Number of iterations: 1307  loss: 0.00030987803\n",
      "Number of iterations: 1308  loss: 0.0016432168\n",
      "Number of iterations: 1309  loss: 0.00031050434\n",
      "Number of iterations: 1310  loss: 0.0016433804\n",
      "Number of iterations: 1311  loss: 0.00031112946\n",
      "Number of iterations: 1312  loss: 0.0016435385\n",
      "Number of iterations: 1313  loss: 0.0003117533\n",
      "Number of iterations: 1314  loss: 0.0016436933\n",
      "Number of iterations: 1315  loss: 0.00031237616\n",
      "Number of iterations: 1316  loss: 0.0016438513\n",
      "Number of iterations: 1317  loss: 0.00031299656\n",
      "Number of iterations: 1318  loss: 0.0016440016\n",
      "Number of iterations: 1319  loss: 0.0003136165\n",
      "Number of iterations: 1320  loss: 0.0016441472\n",
      "Number of iterations: 1321  loss: 0.00031423493\n",
      "Number of iterations: 1322  loss: 0.001644282\n",
      "Number of iterations: 1323  loss: 0.00031485423\n",
      "Number of iterations: 1324  loss: 0.0016444179\n",
      "Number of iterations: 1325  loss: 0.00031547216\n",
      "Number of iterations: 1326  loss: 0.0016445501\n",
      "Number of iterations: 1327  loss: 0.00031608483\n",
      "Number of iterations: 1328  loss: 0.0016446736\n",
      "Number of iterations: 1329  loss: 0.00031670046\n",
      "Number of iterations: 1330  loss: 0.0016447939\n",
      "Number of iterations: 1331  loss: 0.0003173161\n",
      "Number of iterations: 1332  loss: 0.0016449115\n",
      "Number of iterations: 1333  loss: 0.00031792725\n",
      "Number of iterations: 1334  loss: 0.0016450161\n",
      "Number of iterations: 1335  loss: 0.0003185394\n",
      "Number of iterations: 1336  loss: 0.0016451234\n",
      "Number of iterations: 1337  loss: 0.00031914905\n",
      "Number of iterations: 1338  loss: 0.0016452175\n",
      "Number of iterations: 1339  loss: 0.00031975823\n",
      "Number of iterations: 1340  loss: 0.0016453179\n",
      "Number of iterations: 1341  loss: 0.00032036458\n",
      "Number of iterations: 1342  loss: 0.001645406\n",
      "Number of iterations: 1343  loss: 0.00032097427\n",
      "Number of iterations: 1344  loss: 0.0016454909\n",
      "Number of iterations: 1345  loss: 0.00032157623\n",
      "Number of iterations: 1346  loss: 0.0016455733\n",
      "Number of iterations: 1347  loss: 0.0003221811\n",
      "Number of iterations: 1348  loss: 0.0016456473\n",
      "Number of iterations: 1349  loss: 0.0003227832\n",
      "Number of iterations: 1350  loss: 0.0016457185\n",
      "Number of iterations: 1351  loss: 0.0003233834\n",
      "Number of iterations: 1352  loss: 0.0016457883\n",
      "Number of iterations: 1353  loss: 0.00032398422\n",
      "Number of iterations: 1354  loss: 0.0016458444\n",
      "Number of iterations: 1355  loss: 0.0003245816\n",
      "Number of iterations: 1356  loss: 0.0016459007\n",
      "Number of iterations: 1357  loss: 0.0003251766\n",
      "Number of iterations: 1358  loss: 0.0016459569\n",
      "Number of iterations: 1359  loss: 0.00032577285\n",
      "Number of iterations: 1360  loss: 0.0016460017\n",
      "Number of iterations: 1361  loss: 0.00032636736\n",
      "Number of iterations: 1362  loss: 0.0016460485\n",
      "Number of iterations: 1363  loss: 0.00032695694\n",
      "Number of iterations: 1364  loss: 0.0016460941\n",
      "Number of iterations: 1365  loss: 0.0003275477\n",
      "Number of iterations: 1366  loss: 0.0016461333\n",
      "Number of iterations: 1367  loss: 0.00032813568\n",
      "Number of iterations: 1368  loss: 0.0016461547\n",
      "Number of iterations: 1369  loss: 0.00032871976\n",
      "Number of iterations: 1370  loss: 0.0016461808\n",
      "Number of iterations: 1371  loss: 0.00032930766\n",
      "Number of iterations: 1372  loss: 0.0016461968\n",
      "Number of iterations: 1373  loss: 0.0003298931\n",
      "Number of iterations: 1374  loss: 0.0016462114\n",
      "Number of iterations: 1375  loss: 0.00033047507\n",
      "Number of iterations: 1376  loss: 0.0016462154\n",
      "Number of iterations: 1377  loss: 0.00033105645\n",
      "Number of iterations: 1378  loss: 0.0016462124\n",
      "Number of iterations: 1379  loss: 0.00033164103\n",
      "Number of iterations: 1380  loss: 0.0016462082\n",
      "Number of iterations: 1381  loss: 0.00033221932\n",
      "Number of iterations: 1382  loss: 0.0016462012\n",
      "Number of iterations: 1383  loss: 0.00033279505\n",
      "Number of iterations: 1384  loss: 0.0016461948\n",
      "Number of iterations: 1385  loss: 0.00033337207\n",
      "Number of iterations: 1386  loss: 0.0016461771\n",
      "Number of iterations: 1387  loss: 0.00033394428\n",
      "Number of iterations: 1388  loss: 0.0016461688\n",
      "Number of iterations: 1389  loss: 0.0003345152\n",
      "Number of iterations: 1390  loss: 0.0016461413\n",
      "Number of iterations: 1391  loss: 0.00033508777\n",
      "Number of iterations: 1392  loss: 0.0016461137\n",
      "Number of iterations: 1393  loss: 0.00033565445\n",
      "Number of iterations: 1394  loss: 0.001646086\n",
      "Number of iterations: 1395  loss: 0.00033622116\n",
      "Number of iterations: 1396  loss: 0.0016460533\n",
      "Number of iterations: 1397  loss: 0.00033678606\n",
      "Number of iterations: 1398  loss: 0.0016460065\n",
      "Number of iterations: 1399  loss: 0.0003373488\n",
      "Number of iterations: 1400  loss: 0.0016459709\n",
      "Number of iterations: 1401  loss: 0.00033791043\n",
      "Number of iterations: 1402  loss: 0.0016459223\n",
      "Number of iterations: 1403  loss: 0.00033847088\n",
      "Number of iterations: 1404  loss: 0.0016458652\n",
      "Number of iterations: 1405  loss: 0.000339034\n",
      "Number of iterations: 1406  loss: 0.0016458071\n",
      "Number of iterations: 1407  loss: 0.00033958835\n",
      "Number of iterations: 1408  loss: 0.0016457477\n",
      "Number of iterations: 1409  loss: 0.00034014523\n",
      "Number of iterations: 1410  loss: 0.0016456841\n",
      "Number of iterations: 1411  loss: 0.00034069948\n",
      "Number of iterations: 1412  loss: 0.0016456207\n",
      "Number of iterations: 1413  loss: 0.000341255\n",
      "Number of iterations: 1414  loss: 0.001645544\n",
      "Number of iterations: 1415  loss: 0.00034180665\n",
      "Number of iterations: 1416  loss: 0.0016454718\n",
      "Number of iterations: 1417  loss: 0.0003423567\n",
      "Number of iterations: 1418  loss: 0.00164538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 1419  loss: 0.00034290762\n",
      "Number of iterations: 1420  loss: 0.0016452933\n",
      "Number of iterations: 1421  loss: 0.00034345596\n",
      "Number of iterations: 1422  loss: 0.0016452029\n",
      "Number of iterations: 1423  loss: 0.00034400035\n",
      "Number of iterations: 1424  loss: 0.0016451095\n",
      "Number of iterations: 1425  loss: 0.00034454564\n",
      "Number of iterations: 1426  loss: 0.001645009\n",
      "Number of iterations: 1427  loss: 0.0003450849\n",
      "Number of iterations: 1428  loss: 0.0016449166\n",
      "Number of iterations: 1429  loss: 0.0003456309\n",
      "Number of iterations: 1430  loss: 0.0016448015\n",
      "Number of iterations: 1431  loss: 0.00034615834\n",
      "Number of iterations: 1432  loss: 0.001644725\n",
      "Number of iterations: 1433  loss: 0.00034671603\n",
      "Number of iterations: 1434  loss: 0.0016445623\n",
      "Number of iterations: 1435  loss: 0.00034721216\n",
      "Number of iterations: 1436  loss: 0.0016445654\n",
      "Number of iterations: 1437  loss: 0.00034782136\n",
      "Number of iterations: 1438  loss: 0.0016442778\n",
      "Number of iterations: 1439  loss: 0.000348202\n",
      "Number of iterations: 1440  loss: 0.0016445745\n",
      "Number of iterations: 1441  loss: 0.00034896543\n",
      "Number of iterations: 1442  loss: 0.0016440327\n",
      "Number of iterations: 1443  loss: 0.0003491035\n",
      "Number of iterations: 1444  loss: 0.0016446123\n",
      "Number of iterations: 1445  loss: 0.00035003133\n",
      "Number of iterations: 1446  loss: 0.0016438235\n",
      "Number of iterations: 1447  loss: 0.00035024498\n",
      "Number of iterations: 1448  loss: 0.00164391\n",
      "Number of iterations: 1449  loss: 0.0003510108\n",
      "Number of iterations: 1450  loss: 0.0016435471\n",
      "Number of iterations: 1451  loss: 0.00035142433\n",
      "Number of iterations: 1452  loss: 0.0016434684\n",
      "Number of iterations: 1453  loss: 0.00035203947\n",
      "Number of iterations: 1454  loss: 0.0016432207\n",
      "Number of iterations: 1455  loss: 0.00035252408\n",
      "Number of iterations: 1456  loss: 0.0016430987\n",
      "Number of iterations: 1457  loss: 0.00035308753\n",
      "Number of iterations: 1458  loss: 0.0016428956\n",
      "Number of iterations: 1459  loss: 0.000353581\n",
      "Number of iterations: 1460  loss: 0.0016427727\n",
      "Number of iterations: 1461  loss: 0.00035412423\n",
      "Number of iterations: 1462  loss: 0.0016425895\n",
      "Number of iterations: 1463  loss: 0.00035461228\n",
      "Number of iterations: 1464  loss: 0.0016424886\n",
      "Number of iterations: 1465  loss: 0.0003551441\n",
      "Number of iterations: 1466  loss: 0.0016423251\n",
      "Number of iterations: 1467  loss: 0.0003556193\n",
      "Number of iterations: 1468  loss: 0.0016422458\n",
      "Number of iterations: 1469  loss: 0.00035615536\n",
      "Number of iterations: 1470  loss: 0.0016420612\n",
      "Number of iterations: 1471  loss: 0.00035661476\n",
      "Number of iterations: 1472  loss: 0.0016420085\n",
      "Number of iterations: 1473  loss: 0.00035716782\n",
      "Number of iterations: 1474  loss: 0.0016417665\n",
      "Number of iterations: 1475  loss: 0.0003575942\n",
      "Number of iterations: 1476  loss: 0.0016417807\n",
      "Number of iterations: 1477  loss: 0.0003581984\n",
      "Number of iterations: 1478  loss: 0.0016414119\n",
      "Number of iterations: 1479  loss: 0.0003585427\n",
      "Number of iterations: 1480  loss: 0.0016416095\n",
      "Number of iterations: 1481  loss: 0.00035925137\n",
      "Number of iterations: 1482  loss: 0.0016410492\n",
      "Number of iterations: 1483  loss: 0.00035946284\n",
      "Number of iterations: 1484  loss: 0.0016413842\n",
      "Number of iterations: 1485  loss: 0.00036027175\n",
      "Number of iterations: 1486  loss: 0.0016406745\n",
      "Number of iterations: 1487  loss: 0.00036050126\n",
      "Number of iterations: 1488  loss: 0.0016407511\n",
      "Number of iterations: 1489  loss: 0.0003612431\n",
      "Number of iterations: 1490  loss: 0.0016402418\n",
      "Number of iterations: 1491  loss: 0.0003615977\n",
      "Number of iterations: 1492  loss: 0.001640158\n",
      "Number of iterations: 1493  loss: 0.00036223241\n",
      "Number of iterations: 1494  loss: 0.0016397842\n",
      "Number of iterations: 1495  loss: 0.0003626447\n",
      "Number of iterations: 1496  loss: 0.0016396655\n",
      "Number of iterations: 1497  loss: 0.00036322552\n",
      "Number of iterations: 1498  loss: 0.0016393459\n",
      "Number of iterations: 1499  loss: 0.00036364837\n",
      "Number of iterations: 1500  loss: 0.0016392389\n",
      "Number of iterations: 1501  loss: 0.0003642062\n",
      "Number of iterations: 1502  loss: 0.0016389497\n",
      "Number of iterations: 1503  loss: 0.00036461584\n",
      "Number of iterations: 1504  loss: 0.0016388947\n",
      "Number of iterations: 1505  loss: 0.00036517784\n",
      "Number of iterations: 1506  loss: 0.0016385857\n",
      "Number of iterations: 1507  loss: 0.00036555194\n",
      "Number of iterations: 1508  loss: 0.0016386163\n",
      "Number of iterations: 1509  loss: 0.00036614764\n",
      "Number of iterations: 1510  loss: 0.0016382235\n",
      "Number of iterations: 1511  loss: 0.00036645742\n",
      "Number of iterations: 1512  loss: 0.0016383936\n",
      "Number of iterations: 1513  loss: 0.00036712384\n",
      "Number of iterations: 1514  loss: 0.0016378583\n",
      "Number of iterations: 1515  loss: 0.00036735242\n",
      "Number of iterations: 1516  loss: 0.001638104\n",
      "Number of iterations: 1517  loss: 0.0003680841\n",
      "Number of iterations: 1518  loss: 0.0016374631\n",
      "Number of iterations: 1519  loss: 0.00036832326\n",
      "Number of iterations: 1520  loss: 0.0016375395\n",
      "Number of iterations: 1521  loss: 0.00036902618\n",
      "Number of iterations: 1522  loss: 0.0016369914\n",
      "Number of iterations: 1523  loss: 0.0003693447\n",
      "Number of iterations: 1524  loss: 0.0016369236\n",
      "Number of iterations: 1525  loss: 0.0003699776\n",
      "Number of iterations: 1526  loss: 0.0016364672\n",
      "Number of iterations: 1527  loss: 0.00037034642\n",
      "Number of iterations: 1528  loss: 0.0016363495\n",
      "Number of iterations: 1529  loss: 0.00037094025\n",
      "Number of iterations: 1530  loss: 0.0016359222\n",
      "Number of iterations: 1531  loss: 0.0003713171\n",
      "Number of iterations: 1532  loss: 0.0016358326\n",
      "Number of iterations: 1533  loss: 0.0003718992\n",
      "Number of iterations: 1534  loss: 0.001635409\n",
      "Number of iterations: 1535  loss: 0.00037225426\n",
      "Number of iterations: 1536  loss: 0.0016353851\n",
      "Number of iterations: 1537  loss: 0.00037285144\n",
      "Number of iterations: 1538  loss: 0.001634932\n",
      "Number of iterations: 1539  loss: 0.00037315596\n",
      "Number of iterations: 1540  loss: 0.0016350347\n",
      "Number of iterations: 1541  loss: 0.0003737907\n",
      "Number of iterations: 1542  loss: 0.0016345105\n",
      "Number of iterations: 1543  loss: 0.00037402828\n",
      "Number of iterations: 1544  loss: 0.0016346915\n",
      "Number of iterations: 1545  loss: 0.00037470987\n",
      "Number of iterations: 1546  loss: 0.0016341024\n",
      "Number of iterations: 1547  loss: 0.00037494308\n",
      "Number of iterations: 1548  loss: 0.0016341901\n",
      "Number of iterations: 1549  loss: 0.00037561526\n",
      "Number of iterations: 1550  loss: 0.001633631\n",
      "Number of iterations: 1551  loss: 0.0003759001\n",
      "Number of iterations: 1552  loss: 0.0016336143\n",
      "Number of iterations: 1553  loss: 0.0003765264\n",
      "Number of iterations: 1554  loss: 0.0016331152\n",
      "Number of iterations: 1555  loss: 0.0003768551\n",
      "Number of iterations: 1556  loss: 0.0016330471\n",
      "Number of iterations: 1557  loss: 0.00037745526\n",
      "Number of iterations: 1558  loss: 0.0016325697\n",
      "Number of iterations: 1559  loss: 0.0003777926\n",
      "Number of iterations: 1560  loss: 0.0016325023\n",
      "Number of iterations: 1561  loss: 0.0003783884\n",
      "Number of iterations: 1562  loss: 0.0016320205\n",
      "Number of iterations: 1563  loss: 0.00037870425\n",
      "Number of iterations: 1564  loss: 0.0016320095\n",
      "Number of iterations: 1565  loss: 0.00037931767\n",
      "Number of iterations: 1566  loss: 0.0016314829\n",
      "Number of iterations: 1567  loss: 0.0003795868\n",
      "Number of iterations: 1568  loss: 0.0016315529\n",
      "Number of iterations: 1569  loss: 0.00038023878\n",
      "Number of iterations: 1570  loss: 0.0016309734\n",
      "Number of iterations: 1571  loss: 0.00038047673\n",
      "Number of iterations: 1572  loss: 0.0016310608\n",
      "Number of iterations: 1573  loss: 0.00038114053\n",
      "Number of iterations: 1574  loss: 0.0016304662\n",
      "Number of iterations: 1575  loss: 0.00038138675\n",
      "Number of iterations: 1576  loss: 0.001630487\n",
      "Number of iterations: 1577  loss: 0.00038203146\n",
      "Number of iterations: 1578  loss: 0.0016299374\n",
      "Number of iterations: 1579  loss: 0.00038231557\n",
      "Number of iterations: 1580  loss: 0.0016298944\n",
      "Number of iterations: 1581  loss: 0.00038292469\n",
      "Number of iterations: 1582  loss: 0.0016293946\n",
      "Number of iterations: 1583  loss: 0.00038323394\n",
      "Number of iterations: 1584  loss: 0.0016293317\n",
      "Number of iterations: 1585  loss: 0.00038382618\n",
      "Number of iterations: 1586  loss: 0.0016288398\n",
      "Number of iterations: 1587  loss: 0.00038413244\n",
      "Number of iterations: 1588  loss: 0.0016287965\n",
      "Number of iterations: 1589  loss: 0.00038472746\n",
      "Number of iterations: 1590  loss: 0.0016282846\n",
      "Number of iterations: 1591  loss: 0.00038501053\n",
      "Number of iterations: 1592  loss: 0.0016282892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 1593  loss: 0.0003856285\n",
      "Number of iterations: 1594  loss: 0.001627735\n",
      "Number of iterations: 1595  loss: 0.00038587968\n",
      "Number of iterations: 1596  loss: 0.001627777\n",
      "Number of iterations: 1597  loss: 0.00038652067\n",
      "Number of iterations: 1598  loss: 0.0016271919\n",
      "Number of iterations: 1599  loss: 0.0003867647\n",
      "Number of iterations: 1600  loss: 0.0016272052\n",
      "Number of iterations: 1601  loss: 0.0003874022\n",
      "Number of iterations: 1602  loss: 0.001626633\n",
      "Number of iterations: 1603  loss: 0.00038766794\n",
      "Number of iterations: 1604  loss: 0.0016265883\n",
      "Number of iterations: 1605  loss: 0.0003882843\n",
      "Number of iterations: 1606  loss: 0.0016260525\n",
      "Number of iterations: 1607  loss: 0.00038857138\n",
      "Number of iterations: 1608  loss: 0.0016259822\n",
      "Number of iterations: 1609  loss: 0.00038916245\n",
      "Number of iterations: 1610  loss: 0.0016254727\n",
      "Number of iterations: 1611  loss: 0.00038945675\n",
      "Number of iterations: 1612  loss: 0.0016253998\n",
      "Number of iterations: 1613  loss: 0.00039004648\n",
      "Number of iterations: 1614  loss: 0.0016248788\n",
      "Number of iterations: 1615  loss: 0.0003903328\n",
      "Number of iterations: 1616  loss: 0.0016248261\n",
      "Number of iterations: 1617  loss: 0.00039093214\n",
      "Number of iterations: 1618  loss: 0.0016242911\n",
      "Number of iterations: 1619  loss: 0.00039119186\n",
      "Number of iterations: 1620  loss: 0.0016242875\n",
      "Number of iterations: 1621  loss: 0.00039180723\n",
      "Number of iterations: 1622  loss: 0.0016237266\n",
      "Number of iterations: 1623  loss: 0.0003920539\n",
      "Number of iterations: 1624  loss: 0.0016237224\n",
      "Number of iterations: 1625  loss: 0.00039267333\n",
      "Number of iterations: 1626  loss: 0.0016231518\n",
      "Number of iterations: 1627  loss: 0.00039292782\n",
      "Number of iterations: 1628  loss: 0.0016231156\n",
      "Number of iterations: 1629  loss: 0.000393536\n",
      "Number of iterations: 1630  loss: 0.0016225565\n",
      "Number of iterations: 1631  loss: 0.00039380763\n",
      "Number of iterations: 1632  loss: 0.0016224893\n",
      "Number of iterations: 1633  loss: 0.00039439826\n",
      "Number of iterations: 1634  loss: 0.0016219548\n",
      "Number of iterations: 1635  loss: 0.00039468243\n",
      "Number of iterations: 1636  loss: 0.0016218709\n",
      "Number of iterations: 1637  loss: 0.00039526323\n",
      "Number of iterations: 1638  loss: 0.0016213417\n",
      "Number of iterations: 1639  loss: 0.00039554317\n",
      "Number of iterations: 1640  loss: 0.001621275\n",
      "Number of iterations: 1641  loss: 0.00039612182\n",
      "Number of iterations: 1642  loss: 0.0016207312\n",
      "Number of iterations: 1643  loss: 0.0003963925\n",
      "Number of iterations: 1644  loss: 0.0016206838\n",
      "Number of iterations: 1645  loss: 0.000396981\n",
      "Number of iterations: 1646  loss: 0.0016201258\n",
      "Number of iterations: 1647  loss: 0.00039723562\n",
      "Number of iterations: 1648  loss: 0.0016200838\n",
      "Number of iterations: 1649  loss: 0.00039783234\n",
      "Number of iterations: 1650  loss: 0.0016195098\n",
      "Number of iterations: 1651  loss: 0.00039808298\n",
      "Number of iterations: 1652  loss: 0.0016194544\n",
      "Number of iterations: 1653  loss: 0.00039867676\n",
      "Number of iterations: 1654  loss: 0.0016188781\n",
      "Number of iterations: 1655  loss: 0.000398936\n",
      "Number of iterations: 1656  loss: 0.0016188005\n",
      "Number of iterations: 1657  loss: 0.00039951602\n",
      "Number of iterations: 1658  loss: 0.0016182477\n",
      "Number of iterations: 1659  loss: 0.00039978608\n",
      "Number of iterations: 1660  loss: 0.0016181568\n",
      "Number of iterations: 1661  loss: 0.00040035415\n",
      "Number of iterations: 1662  loss: 0.001617608\n",
      "Number of iterations: 1663  loss: 0.00040062796\n",
      "Number of iterations: 1664  loss: 0.0016175059\n",
      "Number of iterations: 1665  loss: 0.00040119284\n",
      "Number of iterations: 1666  loss: 0.0016169593\n",
      "Number of iterations: 1667  loss: 0.00040146057\n",
      "Number of iterations: 1668  loss: 0.0016168732\n",
      "Number of iterations: 1669  loss: 0.00040202547\n",
      "Number of iterations: 1670  loss: 0.0016163186\n",
      "Number of iterations: 1671  loss: 0.0004022798\n",
      "Number of iterations: 1672  loss: 0.0016162462\n",
      "Number of iterations: 1673  loss: 0.0004028529\n",
      "Number of iterations: 1674  loss: 0.0016156718\n",
      "Number of iterations: 1675  loss: 0.00040309964\n",
      "Number of iterations: 1676  loss: 0.0016155882\n",
      "Number of iterations: 1677  loss: 0.00040367566\n",
      "Number of iterations: 1678  loss: 0.0016150065\n",
      "Number of iterations: 1679  loss: 0.0004039269\n",
      "Number of iterations: 1680  loss: 0.0016149111\n",
      "Number of iterations: 1681  loss: 0.0004044937\n",
      "Number of iterations: 1682  loss: 0.0016143386\n",
      "Number of iterations: 1683  loss: 0.00040475305\n",
      "Number of iterations: 1684  loss: 0.0016142196\n",
      "Number of iterations: 1685  loss: 0.0004053094\n",
      "Number of iterations: 1686  loss: 0.0016136623\n",
      "Number of iterations: 1687  loss: 0.00040557538\n",
      "Number of iterations: 1688  loss: 0.0016135285\n",
      "Number of iterations: 1689  loss: 0.0004061214\n",
      "Number of iterations: 1690  loss: 0.0016129642\n",
      "Number of iterations: 1691  loss: 0.00040638246\n",
      "Number of iterations: 1692  loss: 0.0016128443\n",
      "Number of iterations: 1693  loss: 0.0004069286\n",
      "Number of iterations: 1694  loss: 0.0016122751\n",
      "Number of iterations: 1695  loss: 0.00040718314\n",
      "Number of iterations: 1696  loss: 0.0016121682\n",
      "Number of iterations: 1697  loss: 0.0004077295\n",
      "Number of iterations: 1698  loss: 0.0016115914\n",
      "Number of iterations: 1699  loss: 0.0004079759\n",
      "Number of iterations: 1700  loss: 0.0016114868\n",
      "Number of iterations: 1701  loss: 0.0004085245\n",
      "Number of iterations: 1702  loss: 0.0016109033\n",
      "Number of iterations: 1703  loss: 0.00040876764\n",
      "Number of iterations: 1704  loss: 0.0016107883\n",
      "Number of iterations: 1705  loss: 0.00040930975\n",
      "Number of iterations: 1706  loss: 0.0016101984\n",
      "Number of iterations: 1707  loss: 0.0004095579\n",
      "Number of iterations: 1708  loss: 0.0016100634\n",
      "Number of iterations: 1709  loss: 0.00041009733\n",
      "Number of iterations: 1710  loss: 0.0016094806\n",
      "Number of iterations: 1711  loss: 0.00041034963\n",
      "Number of iterations: 1712  loss: 0.0016093317\n",
      "Number of iterations: 1713  loss: 0.00041088182\n",
      "Number of iterations: 1714  loss: 0.001608743\n",
      "Number of iterations: 1715  loss: 0.00041113494\n",
      "Number of iterations: 1716  loss: 0.0016085977\n",
      "Number of iterations: 1717  loss: 0.000411659\n",
      "Number of iterations: 1718  loss: 0.0016080147\n",
      "Number of iterations: 1719  loss: 0.00041190913\n",
      "Number of iterations: 1720  loss: 0.0016078615\n",
      "Number of iterations: 1721  loss: 0.00041243646\n",
      "Number of iterations: 1722  loss: 0.0016072722\n",
      "Number of iterations: 1723  loss: 0.0004126768\n",
      "Number of iterations: 1724  loss: 0.0016071276\n",
      "Number of iterations: 1725  loss: 0.00041320422\n",
      "Number of iterations: 1726  loss: 0.0016065314\n",
      "Number of iterations: 1727  loss: 0.0004134429\n",
      "Number of iterations: 1728  loss: 0.0016063816\n",
      "Number of iterations: 1729  loss: 0.00041396587\n",
      "Number of iterations: 1730  loss: 0.0016057802\n",
      "Number of iterations: 1731  loss: 0.00041420406\n",
      "Number of iterations: 1732  loss: 0.0016056272\n",
      "Number of iterations: 1733  loss: 0.0004147232\n",
      "Number of iterations: 1734  loss: 0.0016050328\n",
      "Number of iterations: 1735  loss: 0.00041496355\n",
      "Number of iterations: 1736  loss: 0.0016048624\n",
      "Number of iterations: 1737  loss: 0.00041547386\n",
      "Number of iterations: 1738  loss: 0.0016042646\n",
      "Number of iterations: 1739  loss: 0.00041571664\n",
      "Number of iterations: 1740  loss: 0.0016040889\n",
      "Number of iterations: 1741  loss: 0.0004162258\n",
      "Number of iterations: 1742  loss: 0.0016034885\n",
      "Number of iterations: 1743  loss: 0.00041646248\n",
      "Number of iterations: 1744  loss: 0.0016033099\n",
      "Number of iterations: 1745  loss: 0.0004169652\n",
      "Number of iterations: 1746  loss: 0.0016026989\n",
      "Number of iterations: 1747  loss: 0.0004172023\n",
      "Number of iterations: 1748  loss: 0.0016025201\n",
      "Number of iterations: 1749  loss: 0.00041770557\n",
      "Number of iterations: 1750  loss: 0.0016019121\n",
      "Number of iterations: 1751  loss: 0.00041793776\n",
      "Number of iterations: 1752  loss: 0.0016017277\n",
      "Number of iterations: 1753  loss: 0.00041843977\n",
      "Number of iterations: 1754  loss: 0.0016011061\n",
      "Number of iterations: 1755  loss: 0.00041866914\n",
      "Number of iterations: 1756  loss: 0.0016009159\n",
      "Number of iterations: 1757  loss: 0.00041916914\n",
      "Number of iterations: 1758  loss: 0.0016002953\n",
      "Number of iterations: 1759  loss: 0.00041939766\n",
      "Number of iterations: 1760  loss: 0.0016000891\n",
      "Number of iterations: 1761  loss: 0.0004198884\n",
      "Number of iterations: 1762  loss: 0.0015994773\n",
      "Number of iterations: 1763  loss: 0.0004201184\n",
      "Number of iterations: 1764  loss: 0.0015992656\n",
      "Number of iterations: 1765  loss: 0.00042060253\n",
      "Number of iterations: 1766  loss: 0.0015986592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 1767  loss: 0.0004208334\n",
      "Number of iterations: 1768  loss: 0.0015984558\n",
      "Number of iterations: 1769  loss: 0.0004213152\n",
      "Number of iterations: 1770  loss: 0.0015978364\n",
      "Number of iterations: 1771  loss: 0.00042153956\n",
      "Number of iterations: 1772  loss: 0.0015976217\n",
      "Number of iterations: 1773  loss: 0.00042201948\n",
      "Number of iterations: 1774  loss: 0.0015969974\n",
      "Number of iterations: 1775  loss: 0.0004222406\n",
      "Number of iterations: 1776  loss: 0.0015967825\n",
      "Number of iterations: 1777  loss: 0.00042271902\n",
      "Number of iterations: 1778  loss: 0.0015961512\n",
      "Number of iterations: 1779  loss: 0.0004229384\n",
      "Number of iterations: 1780  loss: 0.0015959289\n",
      "Number of iterations: 1781  loss: 0.0004234134\n",
      "Number of iterations: 1782  loss: 0.0015952856\n",
      "Number of iterations: 1783  loss: 0.00042363242\n",
      "Number of iterations: 1784  loss: 0.0015950447\n",
      "Number of iterations: 1785  loss: 0.0004241036\n",
      "Number of iterations: 1786  loss: 0.0015944081\n",
      "Number of iterations: 1787  loss: 0.0004243213\n",
      "Number of iterations: 1788  loss: 0.0015941622\n",
      "Number of iterations: 1789  loss: 0.00042478502\n",
      "Number of iterations: 1790  loss: 0.0015935191\n",
      "Number of iterations: 1791  loss: 0.0004250047\n",
      "Number of iterations: 1792  loss: 0.0015932609\n",
      "Number of iterations: 1793  loss: 0.000425465\n",
      "Number of iterations: 1794  loss: 0.0015926202\n",
      "Number of iterations: 1795  loss: 0.0004256793\n",
      "Number of iterations: 1796  loss: 0.0015923724\n",
      "Number of iterations: 1797  loss: 0.000426136\n",
      "Number of iterations: 1798  loss: 0.0015917318\n",
      "Number of iterations: 1799  loss: 0.00042634716\n",
      "Number of iterations: 1800  loss: 0.0015914747\n",
      "Number of iterations: 1801  loss: 0.00042680188\n",
      "Number of iterations: 1802  loss: 0.001590826\n",
      "Number of iterations: 1803  loss: 0.00042700715\n",
      "Number of iterations: 1804  loss: 0.0015905631\n",
      "Number of iterations: 1805  loss: 0.00042745445\n",
      "Number of iterations: 1806  loss: 0.0015899095\n",
      "Number of iterations: 1807  loss: 0.00042766306\n",
      "Number of iterations: 1808  loss: 0.0015896495\n",
      "Number of iterations: 1809  loss: 0.000428108\n",
      "Number of iterations: 1810  loss: 0.0015889893\n",
      "Number of iterations: 1811  loss: 0.00042831668\n",
      "Number of iterations: 1812  loss: 0.0015887051\n",
      "Number of iterations: 1813  loss: 0.00042875725\n",
      "Number of iterations: 1814  loss: 0.0015880406\n",
      "Number of iterations: 1815  loss: 0.0004289641\n",
      "Number of iterations: 1816  loss: 0.0015877488\n",
      "Number of iterations: 1817  loss: 0.0004293965\n",
      "Number of iterations: 1818  loss: 0.0015870837\n",
      "Number of iterations: 1819  loss: 0.000429605\n",
      "Number of iterations: 1820  loss: 0.0015867868\n",
      "Number of iterations: 1821  loss: 0.0004300336\n",
      "Number of iterations: 1822  loss: 0.0015861106\n",
      "Number of iterations: 1823  loss: 0.00043023893\n",
      "Number of iterations: 1824  loss: 0.001585817\n",
      "Number of iterations: 1825  loss: 0.0004306677\n",
      "Number of iterations: 1826  loss: 0.0015851395\n",
      "Number of iterations: 1827  loss: 0.00043086542\n",
      "Number of iterations: 1828  loss: 0.0015848351\n",
      "Number of iterations: 1829  loss: 0.00043128728\n",
      "Number of iterations: 1830  loss: 0.0015841564\n",
      "Number of iterations: 1831  loss: 0.00043148294\n",
      "Number of iterations: 1832  loss: 0.0015838527\n",
      "Number of iterations: 1833  loss: 0.00043190102\n",
      "Number of iterations: 1834  loss: 0.0015831748\n",
      "Number of iterations: 1835  loss: 0.00043209497\n",
      "Number of iterations: 1836  loss: 0.0015828576\n",
      "Number of iterations: 1837  loss: 0.0004325094\n",
      "Number of iterations: 1838  loss: 0.0015821761\n",
      "Number of iterations: 1839  loss: 0.0004327037\n",
      "Number of iterations: 1840  loss: 0.001581839\n",
      "Number of iterations: 1841  loss: 0.00043311637\n",
      "Number of iterations: 1842  loss: 0.0015811509\n",
      "Number of iterations: 1843  loss: 0.00043331046\n",
      "Number of iterations: 1844  loss: 0.0015808214\n",
      "Number of iterations: 1845  loss: 0.00043371873\n",
      "Number of iterations: 1846  loss: 0.001580118\n",
      "Number of iterations: 1847  loss: 0.00043390904\n",
      "Number of iterations: 1848  loss: 0.0015797775\n",
      "Number of iterations: 1849  loss: 0.00043431073\n",
      "Number of iterations: 1850  loss: 0.0015790813\n",
      "Number of iterations: 1851  loss: 0.00043449763\n",
      "Number of iterations: 1852  loss: 0.0015787383\n",
      "Number of iterations: 1853  loss: 0.00043489455\n",
      "Number of iterations: 1854  loss: 0.0015780434\n",
      "Number of iterations: 1855  loss: 0.0004350782\n",
      "Number of iterations: 1856  loss: 0.0015776912\n",
      "Number of iterations: 1857  loss: 0.0004354728\n",
      "Number of iterations: 1858  loss: 0.0015769919\n",
      "Number of iterations: 1859  loss: 0.00043565666\n",
      "Number of iterations: 1860  loss: 0.0015766359\n",
      "Number of iterations: 1861  loss: 0.0004360452\n",
      "Number of iterations: 1862  loss: 0.0015759335\n",
      "Number of iterations: 1863  loss: 0.00043622658\n",
      "Number of iterations: 1864  loss: 0.0015755662\n",
      "Number of iterations: 1865  loss: 0.00043661165\n",
      "Number of iterations: 1866  loss: 0.0015748582\n",
      "Number of iterations: 1867  loss: 0.00043678816\n",
      "Number of iterations: 1868  loss: 0.0015744966\n",
      "Number of iterations: 1869  loss: 0.0004371666\n",
      "Number of iterations: 1870  loss: 0.0015737852\n",
      "Number of iterations: 1871  loss: 0.0004373435\n",
      "Number of iterations: 1872  loss: 0.001573413\n",
      "Number of iterations: 1873  loss: 0.0004377185\n",
      "Number of iterations: 1874  loss: 0.0015726908\n",
      "Number of iterations: 1875  loss: 0.0004378978\n",
      "Number of iterations: 1876  loss: 0.0015723126\n",
      "Number of iterations: 1877  loss: 0.00043827138\n",
      "Number of iterations: 1878  loss: 0.0015715909\n",
      "Number of iterations: 1879  loss: 0.00043844437\n",
      "Number of iterations: 1880  loss: 0.0015712064\n",
      "Number of iterations: 1881  loss: 0.0004388128\n",
      "Number of iterations: 1882  loss: 0.001570478\n",
      "Number of iterations: 1883  loss: 0.00043898588\n",
      "Number of iterations: 1884  loss: 0.001570086\n",
      "Number of iterations: 1885  loss: 0.00043935547\n",
      "Number of iterations: 1886  loss: 0.0015693562\n",
      "Number of iterations: 1887  loss: 0.00043952552\n",
      "Number of iterations: 1888  loss: 0.0015689536\n",
      "Number of iterations: 1889  loss: 0.00043988516\n",
      "Number of iterations: 1890  loss: 0.0015682187\n",
      "Number of iterations: 1891  loss: 0.00044005443\n",
      "Number of iterations: 1892  loss: 0.0015678147\n",
      "Number of iterations: 1893  loss: 0.00044040987\n",
      "Number of iterations: 1894  loss: 0.0015670672\n",
      "Number of iterations: 1895  loss: 0.00044057626\n",
      "Number of iterations: 1896  loss: 0.0015666572\n",
      "Number of iterations: 1897  loss: 0.00044092635\n",
      "Number of iterations: 1898  loss: 0.0015659208\n",
      "Number of iterations: 1899  loss: 0.000441089\n",
      "Number of iterations: 1900  loss: 0.0015655096\n",
      "Number of iterations: 1901  loss: 0.00044143773\n",
      "Number of iterations: 1902  loss: 0.001564765\n",
      "Number of iterations: 1903  loss: 0.00044159425\n",
      "Number of iterations: 1904  loss: 0.0015643566\n",
      "Number of iterations: 1905  loss: 0.00044193817\n",
      "Number of iterations: 1906  loss: 0.0015636103\n",
      "Number of iterations: 1907  loss: 0.0004420959\n",
      "Number of iterations: 1908  loss: 0.0015631852\n",
      "Number of iterations: 1909  loss: 0.00044243355\n",
      "Number of iterations: 1910  loss: 0.0015624353\n",
      "Number of iterations: 1911  loss: 0.00044259135\n",
      "Number of iterations: 1912  loss: 0.0015620058\n",
      "Number of iterations: 1913  loss: 0.0004429268\n",
      "Number of iterations: 1914  loss: 0.0015612546\n",
      "Number of iterations: 1915  loss: 0.00044308268\n",
      "Number of iterations: 1916  loss: 0.001560814\n",
      "Number of iterations: 1917  loss: 0.00044341365\n",
      "Number of iterations: 1918  loss: 0.0015600558\n",
      "Number of iterations: 1919  loss: 0.00044356842\n",
      "Number of iterations: 1920  loss: 0.001559604\n",
      "Number of iterations: 1921  loss: 0.00044389887\n",
      "Number of iterations: 1922  loss: 0.0015588432\n",
      "Number of iterations: 1923  loss: 0.00044404823\n",
      "Number of iterations: 1924  loss: 0.0015583906\n",
      "Number of iterations: 1925  loss: 0.00044437358\n",
      "Number of iterations: 1926  loss: 0.0015576229\n",
      "Number of iterations: 1927  loss: 0.00044452123\n",
      "Number of iterations: 1928  loss: 0.0015571748\n",
      "Number of iterations: 1929  loss: 0.00044483988\n",
      "Number of iterations: 1930  loss: 0.0015564061\n",
      "Number of iterations: 1931  loss: 0.00044498293\n",
      "Number of iterations: 1932  loss: 0.0015559488\n",
      "Number of iterations: 1933  loss: 0.00044529722\n",
      "Number of iterations: 1934  loss: 0.0015551795\n",
      "Number of iterations: 1935  loss: 0.00044544067\n",
      "Number of iterations: 1936  loss: 0.001554716\n",
      "Number of iterations: 1937  loss: 0.0004457521\n",
      "Number of iterations: 1938  loss: 0.0015539423\n",
      "Number of iterations: 1939  loss: 0.0004458977\n",
      "Number of iterations: 1940  loss: 0.0015534738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 1941  loss: 0.00044620238\n",
      "Number of iterations: 1942  loss: 0.0015526991\n",
      "Number of iterations: 1943  loss: 0.00044633914\n",
      "Number of iterations: 1944  loss: 0.0015522239\n",
      "Number of iterations: 1945  loss: 0.00044664394\n",
      "Number of iterations: 1946  loss: 0.0015514469\n",
      "Number of iterations: 1947  loss: 0.00044678437\n",
      "Number of iterations: 1948  loss: 0.0015509591\n",
      "Number of iterations: 1949  loss: 0.00044708123\n",
      "Number of iterations: 1950  loss: 0.0015501732\n",
      "Number of iterations: 1951  loss: 0.00044722002\n",
      "Number of iterations: 1952  loss: 0.0015496796\n",
      "Number of iterations: 1953  loss: 0.0004475124\n",
      "Number of iterations: 1954  loss: 0.0015488929\n",
      "Number of iterations: 1955  loss: 0.00044764607\n",
      "Number of iterations: 1956  loss: 0.001548405\n",
      "Number of iterations: 1957  loss: 0.0004479345\n",
      "Number of iterations: 1958  loss: 0.0015476163\n",
      "Number of iterations: 1959  loss: 0.00044806735\n",
      "Number of iterations: 1960  loss: 0.0015471139\n",
      "Number of iterations: 1961  loss: 0.0004483541\n",
      "Number of iterations: 1962  loss: 0.0015463157\n",
      "Number of iterations: 1963  loss: 0.00044848342\n",
      "Number of iterations: 1964  loss: 0.0015458127\n",
      "Number of iterations: 1965  loss: 0.00044876704\n",
      "Number of iterations: 1966  loss: 0.0015450163\n",
      "Number of iterations: 1967  loss: 0.00044889218\n",
      "Number of iterations: 1968  loss: 0.0015445016\n",
      "Number of iterations: 1969  loss: 0.0004491702\n",
      "Number of iterations: 1970  loss: 0.0015437072\n",
      "Number of iterations: 1971  loss: 0.00044929647\n",
      "Number of iterations: 1972  loss: 0.0015431916\n",
      "Number of iterations: 1973  loss: 0.00044956917\n",
      "Number of iterations: 1974  loss: 0.0015423929\n",
      "Number of iterations: 1975  loss: 0.00044969257\n",
      "Number of iterations: 1976  loss: 0.0015418729\n",
      "Number of iterations: 1977  loss: 0.00044996306\n",
      "Number of iterations: 1978  loss: 0.0015410626\n",
      "Number of iterations: 1979  loss: 0.00045008212\n",
      "Number of iterations: 1980  loss: 0.0015405357\n",
      "Number of iterations: 1981  loss: 0.00045035337\n",
      "Number of iterations: 1982  loss: 0.0015397284\n",
      "Number of iterations: 1983  loss: 0.00045046888\n",
      "Number of iterations: 1984  loss: 0.0015392045\n",
      "Number of iterations: 1985  loss: 0.0004507299\n",
      "Number of iterations: 1986  loss: 0.0015383847\n",
      "Number of iterations: 1987  loss: 0.00045084898\n",
      "Number of iterations: 1988  loss: 0.00153785\n",
      "Number of iterations: 1989  loss: 0.00045110864\n",
      "Number of iterations: 1990  loss: 0.0015370349\n",
      "Number of iterations: 1991  loss: 0.00045122244\n",
      "Number of iterations: 1992  loss: 0.0015364923\n",
      "Number of iterations: 1993  loss: 0.00045147815\n",
      "Number of iterations: 1994  loss: 0.0015356696\n",
      "Number of iterations: 1995  loss: 0.00045159255\n",
      "Number of iterations: 1996  loss: 0.0015351243\n",
      "Number of iterations: 1997  loss: 0.00045184576\n",
      "Number of iterations: 1998  loss: 0.0015343025\n",
      "Number of iterations: 1999  loss: 0.00045195382\n",
      "model_save:  model_save2/modle.ckpt\n",
      "The train has finished\n"
     ]
    }
   ],
   "source": [
    "def train_lstm(batch_size=32,time_step=2,train_begin=0,train_end=1200):\n",
    "    X=tf.placeholder(tf.float32, shape=[None,time_step,input_size])\n",
    "    Y=tf.placeholder(tf.float32, shape=[None,time_step,output_size])\n",
    "    batch_index,train_x,train_y=get_train_data(batch_size,time_step,train_begin,train_end)\n",
    "    with tf.variable_scope(\"sec_lstm\"):\n",
    "        pred,_=lstm(X)\n",
    "    loss=tf.reduce_mean(tf.square(tf.reshape(pred,[-1])-tf.reshape(Y, [-1])))\n",
    "    train_op=tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "    saver=tf.train.Saver(tf.global_variables(),max_to_keep=2)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(2000):    \n",
    "            for step in range(len(batch_index)-1):\n",
    "                _,loss_=sess.run([train_op,loss],feed_dict={X:train_x[batch_index[step]:batch_index[step+1]],Y:train_y[batch_index[step]:batch_index[step+1]],keep_prob:0.5})\n",
    "            print(\"Number of iterations:\",i,\" loss:\",loss_)\n",
    "        print(\"model_save: \",saver.save(sess,'model_save2/modle.ckpt'))\n",
    "        #I run the code on windows 10,so use  'model_save2\\\\modle.ckpt'\n",
    "        #if you run it on Linux,please use  'model_save2/modle.ckpt'\n",
    "        print(\"The train has finished\")\n",
    "train_lstm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model_save2/modle.ckpt\n",
      "196.25127698091475\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD8CAYAAACVZ8iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd4VGX2wPHvgQARBOmIFCkiCihFpNgRpagrYlmxrIg/xbW79rb21cVVUVwVCxYUe2WxICIqFpAovSPSpHelhZDz++PcSSZhkkxCMjPMnM/zzDMz733nzjs3k3vmrVdUFeecc6mtXLwL4JxzLv48GDjnnPNg4JxzzoOBc845PBg455zDg4Fzzjk8GDjnnMODgXPOOTwYOOecA9LiXYDC1K5dW5s0aRLvYjjn3F7l559/XquqdYrzmoQOBk2aNCEjIyPexXDOub2KiCwu7mu8mcg555wHA+eccx4MnHPO4cHAOeccHgycc87hwcA55xweDJxzzuHBAIBdu2DYMFi1Kt4lcc65+PBgAHz5JVx6KbRpA+++G+/SOOdc7HkwAGbMsPuGDeGvf4WPPopveZxzLtY8GAAzZ0LduvDTT9CqFdxyC+zcGe9SOedc7HgwwIJB69ZQoQI88gjMnw/PP5+7fdYs6N8fvv4aVOGrr+Chh2DFCtuenQ1//JF3n9nZMSu+c87tMVHVeJehQB07dtSyXqhOFapVgwEDYMgQe969O0yfDuPGQZUqcMwxsHy55W/QAH7/3R5XrQrnnw9ffAGLF9vjXr3g2Wdh7lwYPx4OOaRMi++cc7sRkZ9VtWNxXpPyNYMlS+DPP61mACACgwdDZiYcfjh07AjbtsGkSfD443DooVZrmD7dgsTzz0Pz5nDFFfDBB3DhhbBsme3r9NNhw4bSL/PWrXZzzrnSkvI1g08/hVNPtV/xxxyTm75+PTz6KHz8sQ077dIl8uszM6FiRXu8erU1OR17LEycCN262e3zzy3IlJRq7uszM60sy5fDSy/BKaeUfL/OueTkNYMSmDnT7kM1g5CaNa1fYObMggMB5AYCsE7obt0gLQ2OPtqCyRdfWF/DnrjiCjjySFi5EgYNgsmToXJlC2L33rtn+3bOOfBgwMyZUL8+1KhR+vu+7DKoVQv++9+S72P9enj5ZcjIsJrLAw9Av37WqX3RRXDffTB6dOmV2TmXmqIKBiJSXUTeE5E5IjJbRLqKyL0i8ruITAlup4Tlv11EFojIXBHpGZbeK0hbICK3lcUHyrFrV1TZQiOJysI++9hkto8+sr6Jknj7bWsaGjLEmqGqVYMnn4T0dBg61Mp+8cWwdm2pFt05l2KirRk8CXyuqocAbYHZQfpgVW0X3D4FEJFWQD+gNdALeEZEyotIeeBpoDfQCjgvyFv6Vq2CDh3gf/+LuFnVTtDz5tkv7LIKBmBNPGAjjEpi+HCbGX311dY89P331hwFFmxGjLDaQ9++vpyGc67kigwGIlINOA4YBqCqmaq6sZCX9AHeUtUdqvobsADoFNwWqOpCVc0E3grylr6KFdlVIR3OPBPef3+3zWPG2MmzZUsblRN1MJg71xrtf/wx6okEBx4IffrAU0/BHXfkDlGNxrx5MGGCzXEQsVFLLVvmzdO2rQWMjAyLf7fcYrOo3347+vdxzrloagbNgDXAyyIyWUReFJEqwbarRWSaiLwkIqFW9wbA0rDXLwvSCkovdasya3DQb2P4rW4n9NxzbcxnmLfesjkCjz0GZ51lHbGF2rED7rnHxpredhscdRQ0bWoN9qFxpIUYPBh69IB//9sCz7x5RX+GWbPsrcqVgwsuKDzvuedafKpSBZ54wuZHXHCBpTnnXFRUtdAb0BHIAjoHz58EHgDqAeWxgPIv4KVg+9PAhWGvHwacBZwDvBiW/jfgqQjvNxDIADIaN26sJbFhg+pll6lW4Q/NqNRVd1WspPr116qqumOHavXqqhddFOXOJk9WbdNGFVTPP1913jzV115T7dFDVcTSW7ZU7dfPtp91luqJJ6qecorquHF5djVrlmrt2qqHHqq6aVNu+vPPq/bsqXrNNapXXmm7A9Xy5VVvuin6z52drbpzp+rGjapNm6o2bqy6bl30r3fOJQcgQ4s4t+e/RRMM9gcWhT0/FvgkX54mwIzg8e3A7WHbRgNdg9vosPQ8+SLdjjjiiD06IN9+q9ogfa2uqHGI6n77qfbtqyu6nKEfcIYuP/ps1aFDVdevj/zinTtVH3xQNS1NtX591U8+2T3PwoWqgwap/uUvqs2aqTZvbmf6o49WPeAAO7w9eqhmZOS8ZNw4O8l37646c6bq449btiZNVKtUsVvv3qpPPqm6cmXJP/ukSaoVKqhef33J9+Gc2zuVSTCw/TIeaBk8vhf4D1A/bPs/sH4CsI7jqUAloCmwMKhBpAWPmwIVgzytC3vfPQ0Gqqpnnql6RJ3Fmt3tRNXDDtPF1Q/T6eUP1+wDD7SPX6mS6g035P0J/euvqp072/Zzz1Vdu7b4b7x1q+qjj6rWrGn7Oecc1TlzVFX1hRdU99nHksEqE5mZqrt2qWZl7fFHznHYYap9+5be/pxze4eyDAbtgqabacBHQA3gNWB6kDYyX3C4E/gVmAv0Dks/BZgXbLuzqPctjWDw5pv2Kb/5RnXLFtV991X9v/9Ta1PJyFAdMMCae6pXtzaZ4cNVa9Sw52++ucfvrxs3qv7zn/aTv3x5a58aO1bXrMzSBx9UvfFGCwQlkpVl1YepU60aNHq06ldf2efaulWPOspqIM651FKSYJD0y1H8+SfUqWPj/cuXtzH6339vfcA5pk+3zuCPPrL5CW3b2iik5s337AOEW7XKpjS/9JIVql49OOccOO886Nq16PUqVG0q82efwQ8/2NKqa9cWPKrpjDPovf1D1q+3pTGcc6mjJMtRJH0wABsxNGaMLTN99dU2zDOiFStsLGfPnrbeQ1nYutUWRHrrLRg1ykYqNW1qw38uuGD3ZU5VbXGk0NraFSvCEUfY5IP997egUq8eVK9uM9GysmzywbBhXNtzLmMWtWD27MhFcc4lJw8GBXjrLfsBfvDBuev6JITNm6028vrrMHas/cpv1Qo6d7b7unXh1VftAgqdO1skO/PMoj/AypXQuDFjW1xO/01PRTP61TmXRDwYFGDrVrtewa232sSshLRihUWtMWNsvezQ+hJVq9qKd5ddVrylT/v3Z8eb73NQpWUs/aN62ZTZOZeQPBgkC1WrNaxZY8un1qxZ/H388gsccQQ38x8G7bqJcim/JKFzqcOXsE4WIrDffnDQQSULBAAdOvD7gUdxCcPY8mfiBnznXGLwYJDEFhxzMYcyh+3fpWDtyjlXLB4MktiaE85hO5VIe2N4vIvinEtwHgySWPr+1fmYPuw76k27KIJzzhXAg0ESq1YNhnMRFTats8lqzjlXAA8GSaxqVRhNT7bvV9cmojnnXAE8GCSxqlVhF2ks6dDXZj1v2xbvIjnnEpQHgyRWrZrdz251FmzZAl98Ed8COecSlgeDJFa1qt3PrX8C1Kix2xXfnHMuxINBEktPh7Q02LilApx+Oowc6aOKnHMReTBIYiJWO/jjD2yBu40bbRls55zLx4NBkqtWzZY5okcPqFLFm4qccxF5MEhyOTWD9HQ49VT48EO7gI9zzoXxYJDkcmoGYFf5Wb3arpTmnHNhPBgkuZyaAUDv3lCpkl3S0znnwngwSHJ5gkHVqnZJzw8+sGsmOOdcwINBksvTTAQ2qmjpUkjFiwY55woUVTAQkeoi8p6IzBGR2SLSVURqisgYEZkf3NcI8oqIDBGRBSIyTUQ6hO2nf5B/voj0L6sP5XLlqRmAzTdIS4N3341bmZxziSfamsGTwOeqegjQFpgN3AaMVdUWwNjgOUBvoEVwGwg8CyAiNYF7gM5AJ+CeUABxZadaNQsGOa1CNWrYqKLhw2HnzriWzTmXOIoMBiJSDTgOGAagqpmquhHoA7waZHsVOCN43AcYrmYCUF1E6gM9gTGqul5VNwBjgF6l+mncbqpWtUCwZUtY4qWXwqpV8MkncSuXcy6xRFMzaAasAV4Wkcki8qKIVAHqqeoKgOC+bpC/AbA07PXLgrSC0l0ZCi1Wl6ffoFcvOOAAeOGFuJTJOZd4ogkGaUAH4FlVbQ9sIbdJKBKJkKaFpOd9schAEckQkYw1a9ZEUTxXmNBidXn6DdLSYMAA+Pxz60x2zqW8aILBMmCZqk4Mnr+HBYdVQfMPwf3qsPyNwl7fEFheSHoeqvq8qnZU1Y516tQpzmdxEUSsGQBccglkZ8Prr8e8TM65xFNkMFDVlcBSEWkZJHUHZgEjgdCIoP7Ax8HjkcBFwaiiLsCmoBlpNNBDRGoEHcc9gjRXhiLWDACaNYNWrXw2snMOsCagaFwDjBCRisBCYAAWSN4Rkf8DlgDnBHk/BU4BFgBbg7yo6noReQCYFOS7X1XXl8qncAUKBYPdagYAHTrAuHExLY9zLjFFFQxUdQrQMcKm7hHyKnBVAft5CXipOAV0eybUTLRbzQCgfXtrJlq1CurVi2m5nHOJxWcgJ7kCm4nAagYAkyfHrDzOucTkwSDJFdiBDNCund3/8kvMyuOcS0weDJJcejqUL19AzaB6detI9pqBcynPg0GSE7Hawdy5sG5dhAwdOnjNwDnnwSAVtGhhlzCoXRueeCLfxg4dYOFC2LAhLmVzziUGDwYp4Jtv4NtvoVEju88j1Ik8ZUrMy+WcSxweDFJAejoceyy0bh1h9Yn27e3em4qcS2keDFJI48awZEm+xLp17TZrVlzK5JxLDB4MUkjjxrB6NWzblm9Dy5bWw+ycS1keDFJI48Z2v2xZvg0eDJxLeR4MUkgoGOzWVHTIIbB2Laz3paKcS1UeDFJIgcGgZbAgrdcOnEtZHgxSSIMGNgnNg4FzLj8PBimkYkXYf/8IwaBpU6hQwYOBcynMg0GKiTi8NC0Nmjf3YOBcCvNgkGIiBgPwEUXOpTgPBikmFAxU86ZnHdSSzNkL+GTkrvgUzDkXVx4MUkzjxrB9u40kDTd+dUsqaiZzRy+KS7mcc/HlwSDFRBpeunMn/PcLG1FUbYU3FTmXijwYpJhIweC11+CbVRYMaqz2YOBcKvJgkGLyB4OsLHj4YTiwQ23WlatNnTW+YJ1zqSiqYCAii0RkuohMEZGMIO1eEfk9SJsiIqeE5b9dRBaIyFwR6RmW3itIWyAit5X+x3FFqVULqlSBSZPs+auvwoIFcPfdsKBia/ZfPzO+BXTOxUVxagbdVLWdqnYMSxscpLVT1U8BRKQV0A9oDfQCnhGR8iJSHnga6A20As4L8roYEoG//x1GjIDRo+H++6FTJzj9dFi4T2sabJy5+1Aj51zSK4tmoj7AW6q6Q1V/AxYAnYLbAlVdqKqZwFtBXhdj999vc8xOP92aix580ILEoqptqJK1OcKyps65ZBdtMFDgCxH5WUQGhqVfLSLTROQlEakRpDUAwq+ntSxIKyjdxVjlyvDii5CZCccfDyedZOnLqrW2BzO9qci5VBNtMDhaVTtgTTxXichxwLNAc6AdsAJ4LMgrEV6vhaTnISIDRSRDRDLWrFkTZfFccZ1wAnzxBbz5ptUKAFbU9GDgXKqKKhio6vLgfjXwIdBJVVep6i5VzQZewJqBwH7xNwp7eUNgeSHp+d/reVXtqKod69SpU9zP44rh5JOhfv3c55lVa7E2rZ4HA+dSUJHBQESqiEjV0GOgBzBDRMJOI/QFZgSPRwL9RKSSiDQFWgA/AZOAFiLSVEQqYp3MI0vvo7g9lZ4O8yu1gRkzis7snEsqaVHkqQd8KNaWkAa8oaqfi8hrItIOa+pZBFwOoKozReQdYBaQBVylqrsARORqYDRQHnhJVf0naAJJT4e5aa3pOmsYZGdDOZ+G4lyqKDIYqOpCoG2E9L8V8pp/Af+KkP4p8Gkxy+hiJD0dZktr2LLFhhk1aRLvIjnnYsR/+rkc6ekwPds7kZ1LRR4MXI70dJia5cHAuVTkwcDlSE+HVTuqQ716fqEb51KMBwOXIz0ddu0CPbglzJsX7+I452LIg4HLkZ5u91nNDvZg4FyK8WDgcoSCQWaTg2H1ati4Mb4Fcs7FjAcDlyMUDLY1OtgeeO3AuZThwcDlCAWDLQ3tqmceDJxLHR4MXI5QMPizbjObfewjipxLGR4MXI6cZqJdFaFpU68ZOJdCPBi4HKFgsH070NKHlzqXSjwYuBx5gsHBwfBSvwSmcynBg4HLsVsw2LoVfv89rmVyzsWGBwOXY7dmIvCmIudShAcDl2O3mgHAnDlxK49zLnY8GLgceYJBgwZQqxb8/HNcy+Sciw0PBi5HnmAgAp06wcSJcS2Tcy42PBi4HHmCAUDnzjBrFmzeHLcyOediw4OByxExGKhCRkbcyuSciw0PBi5HWhqULx8WDDp1sntvKnIu6XkwcHmkp4cFg5o1oUULDwbOpYCogoGILBKR6SIyRUQygrSaIjJGROYH9zWCdBGRISKyQESmiUiHsP30D/LPF5H+ZfOR3J5IT4dt28ISOne2YOAzkZ1LasWpGXRT1Xaq2jF4fhswVlVbAGOD5wC9gRbBbSDwLFjwAO4BOgOdgHtCAcQljjw1A7BgsHIlLF0atzI558renjQT9QFeDR6/CpwRlj5czQSguojUB3oCY1R1vapuAMYAvfbg/V0ZiBgMwJuKnEty0QYDBb4QkZ9FZGCQVk9VVwAE93WD9AZA+M/IZUFaQel5iMhAEckQkYw1a9ZE/0lcqdgtGBx+uPUqT50atzI558peWpT5jlbV5SJSFxgjIoWtUSAR0rSQ9LwJqs8DzwN07NjRG6pjbLdgUKmSLU0xY0bcyuScK3tR1QxUdXlwvxr4EGvzXxU0/xDcrw6yLwMahb28IbC8kHSXQHYLBgCtW8PMmXEpj3MuNooMBiJSRUSqhh4DPYAZwEggNCKoP/Bx8HgkcFEwqqgLsCloRhoN9BCRGkHHcY8gzSWQAoPBr7/mG2bknEsm0TQT1QM+FJFQ/jdU9XMRmQS8IyL/BywBzgnyfwqcAiwAtgIDAFR1vYg8AEwK8t2vqutL7ZO4UpGeDhs25Ets3dqGls6ZA+3bx6VczrmyVWQwUNWFQNsI6euA7hHSFbiqgH29BLxU/GK6WCmwZgDWb+DBwLmk5DOQXR4Rg0GLFlChgvcbOJfEPBi4PCIGgwoV7MpnHgycS1oeDFweEYMB+Igi55KcBwOXR6HB4Lff4M8/Y14m51zZ82Dg8ggFg93WpWvTxu5nz455mZxzZc+DgcsjPR2ysyErK9+G8BFFzrmk48HA5bHb1c5CmjeHypV9jSLnkpQHA5dHgcGgfHlbtG7y5JiXyTlX9jwYuDwKDAZgE86mTLF2JOdcUvFg4PIoMhhs3myjipxzScWDgcujyGAA3lTkXBLyYODyKDQYtGljfQdTpsS0TM65sufBwOVRaDBIT4dWrbxm4FwS8mDg8ig0GAC0a+fBwLkk5MHA5VFkMGjfHlasgFWrYlYm51zZ82Dg8ogqGIDXDpxLMh4MXB5FBoO2wXWOpk+PSXmcc7HhwcDlUWQwqFED6tf35aydSzIeDFweRQYD8GsbOJeEPBi4PELBYMuWQjK1bg2zZvmyFM4lkaiDgYiUF5HJIjIqeP6KiPwmIlOCW7sgXURkiIgsEJFpItIhbB/9RWR+cOtf+h/H7akqVeCgg+CDDyJc0yCkVSvYuhWWLIlp2ZxzZac4NYPrgPxXNrlZVdsFt9C01N5Ai+A2EHgWQERqAvcAnYFOwD0iUmNPCu9KnwjceCNMmgTffltAptC1DbypyLmkEVUwEJGGwKnAi1Fk7wMMVzMBqC4i9YGewBhVXa+qG4AxQK8SltuVof79oU4deOQRWLzYgsP8+WEZPBg4l3SirRk8AdwC5G8k/lfQFDRYRCoFaQ2ApWF5lgVpBaW7BLPPPnDttfDpp3DoofD44/DPf4ZlqF4dDjjAg4FzSaTIYCAipwGrVfXnfJtuBw4BjgRqAreGXhJhN1pIev73GygiGSKSsWbNmqKK58rIlVfCgQfCqafChRdaH8LKlWEZWrWyTmTnXFKIpmZwNHC6iCwC3gJOFJHXVXVF0BS0A3gZ6wcA+8XfKOz1DYHlhaTnoarPq2pHVe1Yp06dYn8gVzpq1oRFi+Ddd+Guu2DnThg2LCyDjyhyLqkUGQxU9XZVbaiqTYB+wFeqemHQD4CICHAGELpS+kjgomBUURdgk6quAEYDPUSkRtBx3CNIcwmuZUvo3h2eew527QoSW7e2EUWLF8e1bM650rEn8wxGiMh0YDpQG3gwSP8UWAgsAF4ArgRQ1fXAA8Ck4HZ/kOb2AldeCUuXwuefBwneiexcUkkrTmZV/Rr4Onh8YgF5FLiqgG0vAS8Vq4QuIXTvbvdz5lg/Aq1aWcKsWXDaaXErl3OudPgMZBeVatXsImfrQ3U5H1HkXFLxYOCiImKdyuvDG/Z8jSLnkoYHAxe1iMFg9mwfUeRcEvBg4KK2WzAIrVHkI4qc2+t5MHBRi1gzAG8qci4JeDBwUYtYMwAPBs4lAQ8GLmq7BYPq1aFBAw8GziUBDwYuajVrwubNtjRFDl+jyLmk4MHARa1mTbvfuDEs0UcUOZcUPBi4qNWqZfe7dSJv3Wqr2rlSk7MGlHMx4sHARS1UM1i3LiwxNKIoIyPm5UlWv/0G++4LP/4Y75K4VOLBwEUtFAzy1Aw6doTGjWHw4EIumuyK45tvYPt2+P77eJfEpRIPBi5qEYNBhQpw220wYQJ89VVcypVsQpWsuXPjWw6XWjwYuKhFDAYAAwZA/frw4IO7vcYV36RJdu/BwMWSBwMXtf32swXrdgsG6elwyy3w9dd2seStW/Ns3rnTNj3zTL5hqW43mZkwdao99mDgYsmDgYtauXJQo0aEYADw979Dv35WOzj0UFi4ELDrHxxwAHTrBlddBS+/HNsy721mzoQdO6BDB1i9Ot8wXufKkAcDVyy7zUIOSU+HN9+03s916+zCycCQIfDnn/D++3DEEfDoozZscskSuPNO+yXscoWaiC680O69duBixYOBK5YCg0HIccfBNdfAW2+xLWMmI0bA2WfDmWdaS9L8+fDGG/CXv8BDD8EPP8Ss6HuFjAyrffXqZc89GLhY8WDgiqVWrSKCAcBNN8G++7LqynvZvBkuvdSSzzwTmjWDiy+G6dMtbdq0sixtya1dC5s2xf59J02y0brNm9uV5TwYuFjxYOCKpciaAVjEuP56mkx6j6dq3sNxh1vDd1oa3HyzrVwxaBDUqZPbWZpIliyxbo+ePWM7dWLbNpgxA448EipWtMDpwaBsZGVZi+aUKfEJ+onIg4ErlqiCATCt5828x1lcvf5+pFlTGD4cVLn8cqsV3HQTtG2beMFg61Y44wz7jBMnwiefWPC66SbrEilLX31lJ6muXe15y5YeDEqTqk3mmzABOneGE06A9u2hdu3EraHGUtTBQETKi8hkERkVPG8qIhNFZL6IvC0iFYP0SsHzBcH2JmH7uD1InysiPUv7w7iyV7OmjXApbO2czEy4+JqqXFX3PTaOmwyHHQb9+8OppyJXXUmbIQORxx7lr1U/o/z0KWStjiK6xMgNN9ivxffft1/m994L998Pjz0GQ4eW7Xu/8ALUrQs9etjzli2tj8XXANxzixbZd3effSzYrlhhI9veftu2v/FGXIuXGFQ1qhtwA/AGMCp4/g7QL3g8FLgieHwlMDR43A94O3jcCpgKVAKaAr8C5Qt7zyOOOEJdYnnySVVQXbfOnmdlqQ4ZorppU26eu++2PB9+qLmZHnlEtVYt1dq17WY/1FRBs9MqqL7zTsw/S347d6pWq6Z68cX2/KWXcou5zz6q++2nmp1dNu/9+++q5cur3nprbtpzz9l7//Zb2bxnKnniCTuWd9+tOnSo6saNudt69lRt3rzs/rbhZs9W/eQT1VWr7H/onXdUR48u/fcBMjTKc3voFm0gaAiMBU4ERgECrAXSgu1dgdHB49FA1+BxWpBPgNuB28P2mZOvoJsHg8Tz2mv2rZk3z56PH2/P//1vez5njp3ULrqoiB2tWaPzXx6vfXlf1xx8lGr58pr9zrtlWvaiTJhgn+Xtt+35zp2qrVurdu2aGwTL6sT84IO2//nzc9O++87Szj1XdfXqsnnfVNG7t2qLFpG3vfCCHefJk8vu/cePV+3ePc9vIBXJfXz55arr16tOmaL6/fd7/n4lCQbRNhM9AdwChCqstYCNqpoVPF8GNAgeNwCWBrWOLGBTkD8nPcJr3F4i/5IUU6bY/Xvv2f2rr9r9I48UsaPatWl03jH8L+1M/nva56xp3pmsv57H5mmLSrvIURs71u67dbP7tDT46ScYP97amKFs+jh+/x1efNHe96CDctOPOsqma7z/vnVoL11a8D5cwbZvtxnwPQtomO7TxyZUvv9+2bz3jTfaiOs5c+Bf/4Jx4+z/47774LvvbMj1c8/Z/1a7dnDllaVfjmikFZVBRE4DVqvqzyJyQig5QlYtYlthrwl/v4HAQIDGjRsXVTwXYwUFg4wMW3p5xAhr865Xr+h9VapkJ7kxE6ry8Zo3mEwT1j73OtWevqtsCl+EsWPh8MNtlFNI5cp2f9hhdsKYMsVOHqVh40Y47zwYPdp+Hw4Zkne7CDzwAHTvboFi/Hg4//zSee9U8t13NlKroGBQp451Jr/7rvUPSaQzVSA72/obGkTxM3b1aptP89NPdoIfNMiWJgd7v5Cjj4aTTrIBCy1b5l5aPNaiqRkcDZwuIouAt7CmoieA6iISCiYNgeXB42VAI4Bg+37A+vD0CK/JoarPq2pHVe1YJ/y/0iWEUDBYs8bup07N/TX7j3/YsMzQ7NlotG1rE8+mbDiQrzmeah8Pj8tS2Nu22ZLR3btH3l65MrRokRv89pQqXHYZfPmlLec0d66dOCLp1MnugxU+XDGNHm2L64afgPM76yz7G5x/fsHXkdi4EU45BRo1shpbVlbkfGA/jI46ykYpffABPP10biCI5OSTbZ/nnJN7iZCYK06bEnACuR3I75K3A/nK4PFV5O1Afid43Jq8HcjiX9orAAAYYElEQVQL8Q7kvc6OHar77mttnDt3qlaqpHrjjapt21rbZ5Uqqn/+Gf3+HnnEXnfaaaqX8KI9mTCh7D5APrNnq/74o+qXX9pbjxpVcN5zz1Vt2rR03vfpp+39HnkkwsY//lB9+GHVmTNzkvbfX/WSS0rnvVPN4YerdutWeJ5t21T/8Q/VXlW+1ee5VEcdN0izxn2rI17dqSefrPrXv1qfQ4UKqr162d+uSxfrb1i5Mu++srNVjztOtXp11R9+KLvPVRjKqgM5J3PeYNAM+AlYEASGSkF6evB8QbC9Wdjr78RGEc0Fehf1fh4MElOfPqoHHqg6Y4Z9g4YPV33gAXv8t78Vb1+//WaBZckS1Wps1J1p6apXXVUWxY6oSxcrd+PG1vEdPioqv4cesrybpi1SHTDAhhsVQ3a26rvv2smkXDnr1Ny1K1+mTZtUjz7a3qhcOdWBA1U3btSjjir6hObyGjtW9cor8w5wKNCGDTbqAXR7WuWcnt111ND3q/bXPo1/0ZYtVb/5xrK/9pr9D4BqerrqiBG5u3rnHUt/7rliFjg723qRJ0xQffZZizQlVObBINY3DwaJ6dln7Zvzz3/a/bRpqr/+qlq3bslHQmRn26+uKYeea0NQd+wo3UIHbrpJtW9fe5yZaTWbNm3s/vjjC3jRtm2qF12k6w/pou9ylmZV2ifnZKEvvxzV+27apHrOObmB54478g5vVFUbotWpk2pamuqwYarXXmsRqnVrvbbPIj3wwJJ95lQ0fbqN1qlSxX68rFlTSOYFC1QPOcSO+x13qG7Zoi8+vFqvqPuezul8kWZXrWp/uJNOsjP88uWqat/ZKVOsFgAWeL75xv6+bdvaiOoiZWerfv216sknq1asmPu9AtUjjyzx5/dg4GLit9/sm1Ojhn1/MzNLZ78HHKD6xMmjbOcff1w6O83n2GMt6GzdakEMVF9/XXXFigJOGDt2qJ56qqqIbu9ynP5KU53X/q924j7pJPv1/re/qd51l377whw9/HDVa65R3b49dxdTplgTQ/ny9gs1Ym3gmmvsZLTvvqoffZS77csvVffbTzdXqadnygeauSMGg+GTwFNP2d924cIiMr7/vv34qFnTTsqRbNhgzXZNmthOy5e3GtuyZapqX5Errsh7Hi9oV7p4seobb9hY4n79VJs1sxfUq6d6ww2qgwerfvCB/ZPtwcQHDwYuZg45xL497duX3j7btVPtc0qmap06qmedVXo7DtOypZV7/HjVV16xx7NmFfKCoOlAhw7V7Gyr/Zx5ZrDtzz9VzzlHd9U/QHdJOV1Pde1Ta7yCaseOqs88Y//z6ekW6L79NsL+x42z9oZy5ay9LH8DtKrqzJm6vkEbVdAtR59sM5Zcofr1U23QoJDz6R9/qJ5xhv1t27XLnThTmOxsq3JcdZX9oqha1dpKA6tX23k8NE8lx5Ilqo8/ntsmGbo1amTf82eftV8npciDgYuZ66+3b09otm5p6NHDWkn02mutyrF+fentPFC9uuZ03F57rWrlyoVU53/4wTLfcUdO0m23WfPDtGn2fPlyO/EfyCJdU+tgzU5P18mXP6O198vM+Z8/6aQI5++pU639Amz6axHta19/uVOvZog1UR12WBHtHq5RI+v0LdCtt9qxHzTIRkIU17x51qvfooW19+3aZdWQSZNs6v3996uefbbqwQfnnvzbt7caxpQpqlu2lPizRcODgYuZzz+3b8/gwaW3zwsvtJq4ZmRoyXrgCrdjR+7/5RlnqB5zjOpRRxWQOTQkpF49+xUZWLfOlqXo08cCQqNG1i49cqTaT8MTTlAFzW7aTDc++oIumbct76/TP/5Qve46iyjVqqned19Uw68WL7Zyj/rHl1bVaNtW9eef9+h4JKslS+xYPflkARkWLrQfG0VOky/Ct99a016rVlabzT+9+KCDrBr58MPR1TxKkQcDFzOZmfbjpzSXSbjhBjuxana26qGH2tm6FC1bZt/4ihXtf3fffVWvvrqAzJ99Zpn/+9/dNoVGTlWpYs0/v/wStjE72xaf6dDBMtWpY2MbGzVSrV/fmhZErLexGDWfrCxrmbjtNrVIXK2a7b9nzzKpQe1tQiO11q5VffNNOzQZGQVkPvtsqxIGbf575JlnrD3qggtUn3/efhVMnJjnB0Q8eDBwe7V//9u+kX/+Gfbk889Lbf+//GK77N079wdcgYOBunSxSQURRjVt3mxBoG1b1aVLC3h9draNbfzrX1VPP93a0y67TPXvfy/xkKvmzW2ug6pa08TDD1tnZnHH8yah0DyRE0+0ztwqVQpo/Rk50jLed1/MyxhLHgzcXm3YMM1dDG7rVhvzWaeOLelZCkJNW0OH5gaDqVMjZJw3zzY++miB+9q4sWRNzXvi5JODPpVwofG9ZTT6am9xzjnWYgN23717hExz5liNqkOHUu+wTTQlCQZ+cRuXMOrWtfvVq7GF599+G7ZssfUtdM+XqFi92u5POAGqVctdG2k3I0bYAjX9+hW4r/32s4XsYqlp0whLUtx1ly2odPnluR8wjKqtidOunV3Kc2+za5ct5VDYRX5WrYIPP4Srr7blHLKybL2fPDZssKsWVaxomffZp0zLvTfyYOASRp5gALZi13/+Y8s8fvXVHu8/tN/69W0Noq5dbc2aPFQtGJxwQnSrkcVQs2Z2Ql+2zNZzUsVObsOH28I5Z59tVxbCVjidN88u1nPbbbaG1ODB8S1/cWVl2TWRrr7aPtrOnZY+blzeS1W++qrlHTgQnn8e/va3fAv6bdxoqycuXGir0fkCmJEVtyoRy5s3E6WW0GS2YcPCErdts0lBoWnDe+CWW2ymcXa29e9FXHrip5+sEC++uMfvV9refltzVqkAm7uU4403VEGzL/+73nhjbjMY2OCls8+2vut162wtpkGDop/T9OKL1tURS9nZ1j8CuTO3H344t/P+uOOsmW77dutLOfbYAna0ebPN5K1QofCFp5IM3mfg9mZ//qmR15G55RY7Ay5Zskf7v/hiG9RTqOuus+FGGzbs0XuVhRUrrC385pttqkGzZvn6t4Ox85fzrA4YYOvlfPmlnVhDs627d89d9eCrr4p+z02bcudmxPICO59+au95//32/KyzrK88tEBcaPmH0FIQ4ZO2c+zcqXrKKfbCFOtT8WDg9npVqtgQ0zwWLrThmHfeuUf77t1btdCv1I4dZTr7uTSFOsOfekp17lzV229XbdsmS0dxiu6UNN319e7Tnc88017TrVvhla21a3OHZYYW5wPVt94qww8UZtcum58VPphr2TKb/T1ggA2zDS1AV7GiDSXdTXa2zSoEm+GbYjwYuL1ekyY2+Ww3p51mZ4PwRX+K6YgjLCAUKLTc5Keflvg9YiU7207qlSppzgiabt1Uhw7aqNktW1pQW7Qoz2t+/92GxWdm2nyFcuVsMtvEiTZwascOqxC1amX7vOUWu1x1jx420e7SS2Pz2UJ/huHD86aHr4G1fbuV77vvIuwgM9PWDgKbKp+CPBi4vV6nTnby2U3op3D4WsHF1KiRav/+hWQ46SRbcjKq5Sbjb/JkGyV5773WhJRjzhw7e7drV+CyB4sWWTA47rjcZqNOnWwCdYUKuStlgK3K0bevLaFU1heNnzvXfhC0bl3CP8O6dTbZACzi7bYqYGrwYOD2eqedZuew3ezaZdP7C1w/onDZ2fYr+pZbCsjw66+aVJORPv3UmtZOPFH1/PPtIgr//rdNrAjO6H37ak4/wssvWwczqL76qu3ilVdy2+xDF+OZP7/sijxypE0DqFWrhBeFmTs39wo0r7xS6uXbm3gwcHu9Sy6x2b0RPf64fWXzrP8QnU2btPB5ZHfcYT+VC5xSvBcaPNhOjI0b57b9gC2f0Lu3Zh7YXP/c7wDdddMtqlOn6m9TN+nnn0X+6T93rpZp8/vmzbZCRPv2u7VuFW78eFu6pFo16yiuXdvSUlxJgoHPM3AJpW5du76yRppjdvHFNlno6aeLvd/QHIPQXIY8MjNh2DDo3RsaNiz2vhPW9dfbxZ0XL4aZM+H33+1zdukCy5ZRoVMHqhzfkXKDH4O2bWnSdj96nlcTjj/eJih8/LFN1sKu/9y4Mbz8Mjz+OHzzzZ4Xb8GCnN3z0UewdSs89RQceGAUL87Ohsces/kgmZkwYADceqtdff6YY/a8cKmouNEjljevGaSe0I//PG3g4S67zFbtLPQiBLk2bbJ9ff+9FrzU0euv28bPPitxufdqy5ZZX8x//mMTCrp2tWMcWn2zfXvVhx7S66/JyqlcVK1qv+ZL6ssvrdmuc2drAezRw/oKouqTWLrU+nfA2rp2u2Scw5uJ3N5u+nRrrbnmmgIyLF+u2bVr68r9D9czem0rsn+wf39bLDSYkxW5halzZ2trTtHOxoi2b7drON53n60eC5rdt6+uW7pFx43TnGGt+WVl2YilUJP9tm02pPXqq3NP9OPHW5NQrVq5TXflyqnedVcRZdqwQfXuu61JqHJlW+K8rHu091IeDFxSuPJKa/4NXUBG1YY9/vijDY98+ZxPVEGf4qoil/QPXVukbVu7323V4tCM4yFDSv1zJJUnnrBaQps2qv/7n3Y6MltbtrT4uWGDLe0/cqT174dqD/fckzu3ASxA/PST1SpatrQLA4VW+gbV2bMLeO+sLOusqFHDMp55Ztn2ZCcBDwYuKaxda5ekPf54GzK+Y4eNeAlfYuHTg69TBX322oKbi/74w85f4a/bbUXqs8+2CxtEXJvC5TFqVM41ezc0bKP3c5c+fsn0PNd1qVHD5gdcfHFu2mOP2ZDVKlVse7NmuUH5228tT8eOBbznpEm2EWwnkyfH7OPuzcokGADpwE/AVGAmcF+Q/grwGzAluLUL0gUYAiwApgEdwvbVH5gf3PoX9d4eDFLXCy/Yt/PII21CcGiZiv/+19Yu2rVytW6TfXRU3QEF7iN01crLL7f76tXzZQj1FSTLcNJYyMxUffFF3XXMsZqFLZL0VbXT9YeHv9YJ3+3Muc7Orl02LDV0JbylSy3AN2wYLFEeZsgQuxR0HnPmWP+QiF1ecsQIbxIqhrIKBgLsGzyuAEwEugTB4OwI+U8BPgte1wWYGKTXBBYG9zWCxzUKe28PBqnt3Xdz18V56KHdt//Y8SrdQQXdMD3ycNBnnrHXLlpkfaBt2oRtnD/fagTHHhv7CxMkic9eW6PfnHS/Zoeab2rWtCt+vfFGxE7dJUus1leopUtV//IXzVlr4rrrvIO4BMq8mQioDPwCdC4kGDwHnBf2fC5QHzgPeK6gfJFuHgzckiWqH3wQ+UfhpHcW6k7K65zTboz42oEDrVkiO9v2M3Nm2MYePWzjHi5+59SGFb3zjl1TuHZtO61UqWKjABYsiH4/b75p0b9yZautrVxZdmVOciUJBlHNMxCR8iIyBVgNjFHVicGmf4nINBEZLCKVgrQGwNKwly8L0gpKd65AjRpB3752rZn82vVtyocVzuXAz4fC8uW7bZ8yxS7qImL7adUq2DB7NnzxBdx8s21we6ZqVbuqzKuvwsqV8P33cNZZMHSoTVA44wz49tuCL1C0YYNdgOC88+CQQ+ziC3ffDfXqxfZzpLiogoGq7lLVdkBDoJOItAFuBw4BjsSafm4Nskf4t0ULSc9DRAaKSIaIZKxZsyaa4rkUlZYG40+6D8nayY/H38YPP+Ruy8qCadOgffsIL3zmGbsozKWXxqysKaN8eTjqKAsMixfDHXfAd9/ZRLaOHe1iRaNHW/BWhbfegtat7aIzDz4I48fDQQfF+1OkpGLNQFbVjcDXQC9VDU0L2gG8DHQKsi0Dwn9uNQSWF5Ke/z2eV9WOqtqxTp06xSmeS0HXPXUQnx1yA10XvMatx09g8WJLnzcPtm+3mkEef/xhJ6pzzwX/fpWt+vXtBL9kidUStm+HW26BXr3sKnLVqllt4IADYMIEuPPO2F9L1OUoMhiISB0RqR483gc4CZgjIvWDNAHOAGYELxkJXCSmC7BJVVcAo4EeIlJDRGoAPYI050qseXM446c72FW3Pv/d9XcG3WHXQ5wyxbbvFgxeecUCwtVXx7ScKa1yZbtG88yZttbIV1/Bk0/CBRfYdSonToQjjoh3KVNeNGG4PvCqiJTHgsc7qjpKRL4SkTpY888U4O9B/k+xEUULgK3AAABVXS8iDwCTgnz3q+r60vsoLmVVrUr5l16gzelncMkb3fm5/2jefbcWFStaE3SOF16AG2+0q6V36lTg7lwZql0bunWzm0soogV16iSAjh07akZGRryL4fYSm9/8hIrnn8VSGvEPBtP6hl4M6jfZrqD+xRcwdiz07Alvvgk1asS7uM6VGRH5WVU7Fus1HgxcMhl163jaPX0pDbfMsxVOt22zDYceau3Td9xhnZzOJbGSBAPvrXFJ5bRBx8KDM+DFF62N+phjbJnj/fePd9GcS2geDFzyqVABrrgi3qVwbq/iF7dxzjnnwcA555wHA+ecc3gwcM45hwcD55xzeDBwzjmHBwPnnHN4MHDOOUeCL0chImuAxXuwi9rA2lIqTmlL5LJBYpcvkcsGiV2+RC4bJHb5ErlskLd8B6pqsdZoT+hgsKdEJKO463PESiKXDRK7fIlcNkjs8iVy2SCxy5fIZYM9L583EznnnPNg4JxzLvmDwfPxLkAhErlskNjlS+SyQWKXL5HLBoldvkQuG+xh+ZK6z8A551x0kr1m4JxzLgpJGQxEpJeIzBWRBSJyWwKUp5GIjBOR2SIyU0SuC9JrisgYEZkf3MftWowiUl5EJovIqOB5UxGZGJTtbRGpGMeyVReR90RkTnAMuybKsRORfwR/0xki8qaIpMfz2InISyKyWkRmhKVFPFZihgT/J9NEpEMcyvaf4O86TUQ+FJHqYdtuD8o2V0R6lmXZCipf2LabRERFpHbwPKbHrrDyicg1wTGaKSKPhKUX7/ipalLdgPLAr0AzoCIwFWgV5zLVBzoEj6sC84BWwCPAbUH6bcCgOJbxBuANYFTw/B2gX/B4KHBFHMv2KnBp8LgiUD0Rjh3QAPgN2CfsmF0cz2MHHAd0AGaEpUU8VsApwGeAAF2AiXEoWw8gLXg8KKxsrYL/3UpA0+B/unysyxekNwJGY3Oeasfj2BVy/LoBXwKVgud1S3r8YvIFjeUN6AqMDnt+O3B7vMuVr4wfAycDc4H6QVp9YG6cytMQGAucCIwKvuBrw/5J8xzTGJetWnDClXzpcT92QTBYCtTErho4CugZ72MHNMl3woh4rIDngPMi5YtV2fJt6wuMCB7n+b8NTsZdY33sgrT3gLbAorBgEPNjV8Df9h3gpAj5in38krGZKPQPGrIsSEsIItIEaA9MBOqp6gqA4L5unIr1BHALkB08rwVsVNWs4Hk8j2EzYA3wctCM9aKIVCEBjp2q/g48CiwBVgCbgJ9JnGMXUtCxSrT/lUuwX9uQIGUTkdOB31V1ar5NCVE+4GDg2KBZ8hsROTJIL3b5kjEYSIS0hBgyJSL7Au8D16vq5niXB0BETgNWq+rP4ckRssbrGKZhVeNnVbU9sAVr6oi7oO29D1YNPwCoAvSOkDUhvn8RJMzfWUTuBLKAEaGkCNliWjYRqQzcCdwdaXOEtHgcuzSgBtZUdTPwjogIJShfMgaDZVgbX0hDYHmcypJDRCpggWCEqn4QJK8SkfrB9vrA6jgU7WjgdBFZBLyFNRU9AVQXkbQgTzyP4TJgmapODJ6/hwWHRDh2JwG/qeoaVd0JfAAcReIcu5CCjlVC/K+ISH/gNOACDdo0EqRszbFAPzX4/2gI/CIi+ydI+QjK8YGan7Dafe2SlC8Zg8EkoEUwoqMi0A8YGc8CBZF6GDBbVR8P2zQS6B887o/1JcSUqt6uqg1VtQl2rL5S1QuAccDZ8SxbUL6VwFIRaRkkdQdmkQDHDmse6iIilYO/cahsCXHswhR0rEYCFwUjY7oAm0LNSbEiIr2AW4HTVXVr2KaRQD8RqSQiTYEWwE+xLJuqTlfVuqraJPj/WIYNBFlJAhy7wEfYDzhE5GBsgMVaSnL8yrrDIx43rKd/HtaDfmcClOcYrIo2DZgS3E7B2ubHAvOD+5pxLucJ5I4mahZ8eRYA7xKMVohTudoBGcHx+wirFifEsQPuA+YAM4DXsNEbcTt2wJtY/8VO7OT1fwUdK6wp4eng/2Q60DEOZVuAtW2H/i+GhuW/MyjbXKB3PI5dvu2LyO1AjumxK+T4VQReD75/vwAnlvT4+Qxk55xzSdlM5Jxzrpg8GDjnnPNg4JxzzoOBc845PBg455zDg4Fzzjk8GDjnnMODgXPOOeD/AaXnpYwoGg2nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prediction(time_step=2):\n",
    "    X=tf.placeholder(tf.float32, shape=[None,time_step,input_size])\n",
    "    mean,std,test_x,test_y=get_test_data(time_step)\n",
    "    with tf.variable_scope(\"sec_lstm\",reuse=tf.AUTO_REUSE):\n",
    "        pred,_=lstm(X)\n",
    "    saver=tf.train.Saver(tf.global_variables())\n",
    "    with tf.Session() as sess:\n",
    "        module_file = tf.train.latest_checkpoint('model_save2')\n",
    "        saver.restore(sess, module_file)\n",
    "        test_predict=[]\n",
    "        for step in range(len(test_x)-1):\n",
    "          prob=sess.run(pred,feed_dict={X:[test_x[step]],keep_prob:1})\n",
    "          predict=prob.reshape((-1))\n",
    "          test_predict.extend(predict)\n",
    "        test_y=np.array(test_y)*std[2]+mean[2]\n",
    "        test_predict=np.array(test_predict)*std[2]+mean[2]\n",
    "        acc=np.average(np.abs(test_predict-test_y[:len(test_predict)])/test_y[:len(test_predict)]) \n",
    "        #print(\"The accuracy of this predict:\",acc)\n",
    "        print(sqrt(mean_squared_error(test_y[:len(test_predict)],test_predict)))\n",
    "        plt.figure()\n",
    "        plt.plot(list(range(len(test_predict))), test_predict, color='b',)\n",
    "        plt.plot(list(range(len(test_y))), test_y,  color='r')\n",
    "        plt.show()\n",
    "\n",
    "prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
